{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czxW4C_gf5Eo"
   },
   "source": [
    "# DATASCI 290 - GenAI - Assignment 5\n",
    "\n",
    "In Assignment 5 you will create and test a RAG system yourself, and write a corresponding business proposal.\n",
    "\n",
    "The overall scenario is as follows:\n",
    "\n",
    "You work at a tech company that is looking for new ways to organize their question answering and search capabilities to accelerate both engineering activity and the marketing team. The company also wants to roll out new GenAI-based products, so a lot of the questions will center around Generative AI concepts. The company has about 300 engineers and a marketing staff of 40. Product releases are done quarterly.\n",
    "\n",
    "Your role is to implement and conduct a (mini-)POC helping the company to evaluate RAG capabilities for the improvement of their document search (and corresponding question answering), supporting particularly the engineering and marketing organizations. You will have a gold dataset with 'good' responses to questions from marketing and engineering teams. You need to develop metric(s) that help you to evaluate how well your RAG system performs relative to the gold data. You should work with the tunables of the setup (LLM, chunking, embeddings, ...) for your iterations.\n",
    "\n",
    "You will also need to write up your findings as a short proposal.\n",
    "\n",
    "(See instructions throughout this notebook.)\n",
    "\n",
    "So overall, the goals of this assignment is for you to:\n",
    "\n",
    "*  To implement a RAG system using LangChain\n",
    "*  Be able to formulate metric(s) that you may want to choose as your evaluation to what degree your system replicates gold answers (labeled data) that we will provide.\n",
    "* Try out various hyper-parameters and settings to see which configuration works the best (given your chosen metric)  \n",
    "* Write a comprehensive evaluation, which also includes risks and limitations (and a lot more)\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Set-Up\n",
    "\n",
    "2. Base RAG components\n",
    "\n",
    "    We will provide a base LangChain-based framework for you to use for your RAG system. The components we’ll need include:  \n",
    "\n",
    "  2.1 Text Embeddings    \n",
    "  2.2 Text Chunking   \n",
    "  2.3 The Vector DB & Semantic Search  \n",
    "  2.4 The Language Model   \n",
    "  2.5 Testing the LLM in a LangChain Chain   \n",
    "  2.6. Setting up a simple RAG Chain     \n",
    "\n",
    "\n",
    "3. Using RAG  \n",
    "  3.1 Loading of Data  \n",
    "  3.2 Test Queries\n",
    "\n",
    "\n",
    "4.  Evaluations\n",
    "\n",
    "  Here, you will conduct your evaluations\n",
    "\n",
    "\n",
    "5. Final Results\n",
    "\n",
    "  In this section you provide the RAG answers to the test questions\n",
    "\n",
    "RULES:  \n",
    "\n",
    "* You can only use the language models specified here  \n",
    "* You can only use the embedding methods we discuss  \n",
    "* You can only use the focuments we provide. And they all must be in your store   \n",
    "* Apart from the provided specifications, some of the things you can freely experiment with include chunk sizes, prompts, etc.\n",
    "\n",
    "\n",
    "**To run this notebook** you should copy it to your personal Colab Pro Google account by uploading it into your Google Drive. From there you can open it as a Colab notebook and run it.  Note it needs a T4 GPU to run.  You may be able to run it in a free Colab notebook.\n",
    "\n",
    "NOTES:\n",
    "* The Open Source Model is not trained for safety. So unsafe answers could be returned.\n",
    "\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We will first install a number of libraries and import what we will need.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwfXUrJEfhnu"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install git+https://github.com/huggingface/transformers\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip install einops\n",
    "!pip install faiss-gpu\n",
    "!pip install langchain_community\n",
    "!pip install --upgrade --quiet chromadb bs4 qdrant-client\n",
    "!pip install langchainhub\n",
    "!pip install -U langchain-huggingface\n",
    "!pip install -U langchain-cohere\n",
    "!pip install --upgrade --quiet  wikipedia\n",
    "!pip install --upgrade --quiet  arxiv\n",
    "!pip install --upgrade --quiet  pymupdf\n",
    "\n",
    "!pip install xmltodict\n",
    "\n",
    "!pip install cohere\n",
    "\n",
    "!pip install git+https://github.com/Tiiiger/bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NjcvYABKieZ",
    "outputId": "63d58be5-f354-44e4-da54-91c8a401e46c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import locale\n",
    "\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PubMedLoader\n",
    "\n",
    "#from langchain_community.chat_models import ChatCohere\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "from google.colab import drive\n",
    "import re\n",
    "import pandas as pd\n",
    "from bert_score import BERTScorer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faZ5fLk_xxAO"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stlb_ciPxxWA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJWr9TkCa7gG"
   },
   "source": [
    "Add your keys from the secret store (do **NOT** print them out or leave them exposed as plaintext in your notebook!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll9IqkVMa7qP"
   },
   "outputs": [],
   "source": [
    "COHERE_API_KEY = userdata.get('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlSHHPW-f3ZL"
   },
   "source": [
    "## 2. Building the Components of our RAG System\n",
    "\n",
    "Let us introduce and test the base components of our RAG system. We will largely use the Hugging Face and LangChan libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N3fqR5vKV9b"
   },
   "source": [
    "### 2.1 The Embedding Model\n",
    "\n",
    "We will need to represent text (pieces) as vectors. For this, we will use the [sentence_transformer]() architecture.\n",
    "\n",
    "\n",
    "\n",
    "**NOTE:** The models you can use are: 'all-mpnet-base-v2', 'all-MiniLM-L6-v2', 'multi-qa-mpnet-base-dot-v1', 'all-distilroberta-v1', and 'avsolatorio/GIST-Embedding-v0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_AqjidjKWif"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# embedding model to represent the text as vectors\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cgzrqje8PN8S",
    "outputId": "eb986a28-c8e0-4680-a26c-3b80bed37baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the text for the query\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "# query text to embedding\n",
    "query_result = base_embeddings.embed_query(text)\n",
    "print(f'Embedding dimension: {len(query_result)}')\n",
    "\n",
    "# define the two documents and convert both of them to embeddings\n",
    "doc_result = base_embeddings.embed_documents([\"Germany won the World Cup 4 times.\", \"This is not a test document.\"])\n",
    "len(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPQGVSGgzG6F"
   },
   "source": [
    "Do those dimensions look correct?\n",
    "\n",
    "Now lets see if the embedding model is working as we want.  Ideally our embeddings go beyond shared words and capture the underlying meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKlCdEeKveqN",
    "outputId": "3901afa9-a8a5-42a3-cce6-d52b357f8c71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21410193, 0.94317828])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how well our embeddng model works\n",
    "\n",
    "# define the cosine similarity between the query and the two documents\n",
    "similarity = cosine_similarity([query_result], doc_result)[0]\n",
    "\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAp4fF5dPhbM"
   },
   "source": [
    "That's how you should define your embedding models.\n",
    "\n",
    "Next, we turn to text chunks.\n",
    "\n",
    "### 2.2. Loading and Chunking Texts\n",
    "\n",
    "We first need to load the documents. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ak5XlaUP1cW"
   },
   "outputs": [],
   "source": [
    "# load the documents (github article about agents)\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZXtDqKcQHlT"
   },
   "source": [
    "We will need to split the  text in chunks that are 'suitable' as retrieval units. Let's for starters define a chunk size of 128 and have no overlap between the chunks:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhcWoTajQnw6",
    "outputId": "7989e7ef-885d-49ed-d6e5-5a65d863199a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits/chunks:  444\n"
     ]
    }
   ],
   "source": [
    "# split the github article into chunks that do not overlap, such that sizes of chunks are all 128.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print('Number of splits/chunks: ', str(len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUrNISkVRB_U"
   },
   "source": [
    "Ok, so it looks like we have now many splits (chunks) from one document. Here is how you can get the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "du9avBkhP1ll",
    "outputId": "d5eb6d98-f612-4ecb-9868-93b2708642b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print one of the chunks\n",
    "splits[50].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-RuP_9xRVtG"
   },
   "source": [
    "Perfect. Now we have the splits and embeddings. Next, the embeddings need to be stored in a vector db.\n",
    "\n",
    "### 2.3 Storing the Embeddings of Chunks in Vectorstores\n",
    "\n",
    "After loading and chunking the data, we need to save the vector representations of the chunks in a vectorstore. We will use Qdrant here for simplicity. We load the splits (structured chunks) and the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T3FVDoJRglN"
   },
   "outputs": [],
   "source": [
    "# saving vector representations of chunks into vectorstore using Qdrant\n",
    "vectorstore = Qdrant.from_documents(splits, # the chunks we just split\n",
    "    base_embeddings, # using the hugging face embedding model\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"test\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar4bpzJtRrio"
   },
   "source": [
    "The nice thing is that the vector store also does the similarity searches for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiSqastIP1oT"
   },
   "outputs": [],
   "source": [
    "# using the vectorstore (into which we just saved the chunks from the article) to do a similarity search to rank the most highly similar chunks\n",
    "query = \"What is Chain of Thought doing?\"\n",
    "docs = vectorstore.similarity_search_by_vector(base_embeddings.embed_query(query)) # will rank the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7CO0lvSR0MA",
    "outputId": "abcc5e60-6448-4140-a25f-0148212dfbe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '6be55c16773944f49ee22af155702aff', '_collection_name': 'test'}, page_content='the model’s thinking process.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '7a5e2d1556484bd09b9c53522616ca15', '_collection_name': 'test'}, page_content='[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'a99ecdf9838c4794949f845e918f39b2', '_collection_name': 'test'}, page_content='the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'c8921b7cb8e4461a9d0110696f64b6a5', '_collection_name': 'test'}, page_content='Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyypf-VZRsxg"
   },
   "source": [
    "Looks good! We have an ordered list of documents that seem to relate to the question. That is what we need.\n",
    "\n",
    "The last major component is the actual LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDDom9EbKXCX"
   },
   "source": [
    "### 2.4. The LLM\n",
    "\n",
    "We will use one Open Source Model (\"mistralai/Mistral-7B-Instruct-v0.1\") and one Proprietery Model (Cohere) for our tests. Let's first set up the OS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YooxnCPNOoQ7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# quantization to reduce precisions of the weights of mistral\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         )\n",
    "\n",
    "# loading the model with the specified quantization\n",
    "llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# loading the tokenizer\n",
    "llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVGOr2cV_q6I"
   },
   "source": [
    "We use the model first to generate a Hugging Face pipeline. A pipeline simplifies the process of actually generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvxo5OKwvjNN"
   },
   "outputs": [],
   "source": [
    "# creating a text generation pipeline\n",
    "mistral_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_mistral_model, # specifying the pre-trained model to use for text generation\n",
    "    tokenizer=llm_mistral_tokenizer, # specifying the tokenizer\n",
    "    max_new_tokens=1000, # sets the maximum number of new tokens to generate.\n",
    "    temperature=0.6, # controls the randomness of predictions\n",
    "    top_p=0.95, # generates more coherent text\n",
    "    do_sample=True, # enables sampling\n",
    "    repetition_penalty=1.2 # penalizes repeated tokens to avoid redundancy in the generated text.\n",
    ")\n",
    "\n",
    "# setting the padding token ID\n",
    "mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n",
    "\n",
    "    # wrapping the Hugging Face pipeline into a LangChain object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOHYPzDiTRIK"
   },
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAUHEvq_TRSv",
    "outputId": "c3c9d490-c87b-4af2-8ac6-024fe5627e26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[INST]Give me a two-sentence story about an apple![/INST]Once upon a time, in the heart of an enchanted orchard, an apple hung from a tree, its radiant red skin shimmering under the sun. A young girl, with eyes full of wonder and dreams, reached up to pluck it, her heart filled with anticipation for the sweet, juicy treasure that awaited her within.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_pipe(\"[INST]Give me a two-sentence story about an apple![/INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh-60AEBTODZ"
   },
   "source": [
    "Reasonable!\n",
    "\n",
    "We will also use a Cohere model, but will create this below as part of the LangChain framework.\n",
    "\n",
    "### 2.5 Testing the LLM in a LangChain Chain\n",
    "\n",
    "Chains will be defined and discussed in Week 11. In short, they are convenient programmatic ways to deal with 'chains' of actions that involve LLMs. For example, a list of events like 'here is a city name. Plug that city name into prompt template, then generate a story about that city. Lastly, format the model output as a string' can be easily handled by LangChain's Chain framework. In this case, the Chain would consist of the prompt template, the LLM, and the String Formatter. The parameter (the city in this case) will be provided at run time by invocation of the Chain. Let's test that.\n",
    "\n",
    "To use a Hugging Face model in a LangChain environment, we need to wrap the model into a LangChain pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1LSyyFmTOYP",
    "outputId": "152a9a89-5877-46bd-b92e-1e4e451c15cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# wrapping the hugging face model in a lang chain pipeline object\n",
    "mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FkQde8yZYqZ"
   },
   "source": [
    "Next, we need to define a template and create a corresponding prompt template that can take any questiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh3R9445K5ct"
   },
   "outputs": [],
   "source": [
    "# setting the template\n",
    "test_llm_template = \"\"\"[INST] Give me a two-sentence story about an {object}! [/INST]\"\"\"\n",
    "\n",
    "# setting the prompt template and specifying the input variable \"object\"\n",
    "test_llm_prompt_template = PromptTemplate(template=test_llm_template, input_variables=[\"object\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V6PO2LISDbT"
   },
   "source": [
    "Let's define a Chain, a static flow of actions that (usually) involve at least a definition of the variables used in the chain, one or more templates, LLM step(s) and potentially other actions. This would be a chain that declares the variable 'object' to be expected when the chain is invoked, then inserts it into the template, and passes this to our mistral model pipeline (wrapped as a LangChain object):    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSvSoD7hZb4Z"
   },
   "outputs": [],
   "source": [
    "# defining the chain which involves the definition of the \"object\" input variable,\n",
    "# the test LLM prompt template, and the langchain version of the hugging face mistral\n",
    "# LLM pipeline\n",
    "\n",
    "test_llm_chain_short = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | mistral_llm_lc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "jH8lUWWSZ-9n",
    "outputId": "02021ff9-75e1-4bd9-93be-2f3a4c357f5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[INST] Give me a two-sentence story about an apple! [/INST] In the heart of an old orchard, an apple hung ripe and red on a weathered tree branch. A young boy climbed up with eager eyes, plucking it to share its sweetness with his grandmother.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking the chain with the input\n",
    "test_llm_chain_short.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYeKmOoMaSKS"
   },
   "source": [
    "Works too. We will use this notation moving forward.\n",
    "\n",
    "Next, how would we do this with a Cohere Chat Model instead of Mistral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JROGul-gZcAU"
   },
   "outputs": [],
   "source": [
    "# defining a cohere chat model\n",
    "cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h3DW_bBfPsI"
   },
   "source": [
    "This can be plugged straight into the Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ5bzWTVaZdO"
   },
   "outputs": [],
   "source": [
    "# plugging the cohere chat model into the chain instead of the mistral one\n",
    "test_cohere_llm_chain_short = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | cohere_chat_model # replaced with cohere\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZldFGowOciCk",
    "outputId": "ad878e15-861d-4905-fdab-b354f9f625f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The apple, once bitter and unloved, fell from the tree and, over time, ripened into something sweet and delicious. Its journey from tree to ground transformed it into something cherished and desired by all.', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c4ff1794-4c65-4516-aed2-f5a2264769df', 'token_count': {'input_tokens': 83, 'output_tokens': 42}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'c4ff1794-4c65-4516-aed2-f5a2264769df', 'token_count': {'input_tokens': 83, 'output_tokens': 42}}, id='run-81524fce-4a95-4286-9e6d-18d1dbce7ed7-0', usage_metadata={'input_tokens': 83, 'output_tokens': 42, 'total_tokens': 125})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cohere_llm_chain_short.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CasSDdMnF8z"
   },
   "source": [
    "Works! (Note: you may want to review the format of the template. The one we used here is the one from Mistral, and the format may or may not be optimal for Cohere.)\n",
    "\n",
    "How can we get the output formatting under control? We can add a String Formatter to the chain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3rSDhR5AeR7_",
    "outputId": "8bb56c68-0434-421c-bf02-99f5d768e4e2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The apple, once bitter and unloved, fell from the tree and transformed into a sweet, juicy delight. Its once tart flesh now offered a burst of flavor, a gift to those who found it.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langchain output parser turns the LLM output into the output string\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# rerunning the llm chain with the output parser\n",
    "test_cohere_llm_chain_short_formatted = (\n",
    "    {\"object\": RunnablePassthrough()}\n",
    "    | test_llm_prompt_template\n",
    "    | cohere_chat_model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "test_cohere_llm_chain_short_formatted.invoke('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry1S5W_ueSWt"
   },
   "source": [
    "### 2.6 Setting Up a Simple RAG Chain\n",
    "\n",
    "For RAG, we will follow the same approach. Except... you will **later** need to change the chain to include the retrieval step.\n",
    "\n",
    "We first do a simple test: create a RAG template that takes a question and a pre-defined context as input, and generates the answer based on the provided context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrSx3gFlncAO",
    "outputId": "50d315d0-f2b2-4786-bf13-69ac7dffb6e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: [INST] Answer the question based only on the following context:\n",
      "{'context': 'Germany has won the World Cup 4 times.', 'question': 'How many times did Germany win the world cup?'}\n",
      "\n",
      "Question: {'context': 'Germany has won the World Cup 4 times.', 'question': 'How many times did Germany win the world cup?'}\n",
      "[/INST]\n",
      "Answer: The answer to your question is consistent with the information provided in the context. According to the context, Germany has won the World Cup a total of 4 times.\n"
     ]
    }
   ],
   "source": [
    "rag_template = \"\"\"[INST] Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "base_rag_chain =(\n",
    "    {\"context\": RunnablePassthrough(),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_template\n",
    "    | mistral_llm_lc\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "predefined_context = \"Germany has won the World Cup 4 times.\"\n",
    "question = \"How many times did Germany win the world cup?\"\n",
    "\n",
    "resp = base_rag_chain.invoke({'context': predefined_context,\n",
    "                           'question': question})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Maq5x1jDhJiX"
   },
   "source": [
    "That's great. But of course, the context needs to be created in an earlier retrieval step. More precisely, the documents will be first retrieved as a list, and then they will need to be formatted into one string to pass to the LLM in the context window.\n",
    "\n",
    "Here is a simple formatting function that can be hooked into the chain, which combines a list of chunks into one string:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VUMkGithJtY"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv0wLvGQhJ5R"
   },
   "source": [
    "So how could we build a simple chain? Let's first just get the retrieval done and the formatted retrieved data and the question inserted into the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UiGHgRLhKEZ"
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Here is a context:\\n{context} \\n\\nand here is a question: \\n{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVRdhLzwjYk6"
   },
   "outputs": [],
   "source": [
    "output = rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8UynrWikgPc"
   },
   "source": [
    "Ok... with some formatting... this looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjlKfsrljYnn",
    "outputId": "f59c3858-d55c-457e-95dc-5b993cd02677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a context:\n",
      "the model’s thinking process.\n",
      "\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "\n",
      "the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes \n",
      "\n",
      "and here is a question: \n",
      "What is Chain of Thought?\n"
     ]
    }
   ],
   "source": [
    "print(output.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx0ZWB7Ul4x8"
   },
   "source": [
    "Let's complete the RAG Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh9ZopW3jYqA"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | mistral_llm_lc\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "_e_gx7lMjYse",
    "outputId": "8ccdb1b9-00e6-4db1-8e01-d1fad4422c2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Human: [INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nthe model’s thinking process.\\n\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n\\nthe problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes \\n\\nHere is a question: \\nWhat is Chain of Thought?.[/INST] According to the context provided, \"Chain of Thought\" (CoT) refers to a method or framework proposed by Wei et al. for eliciting reasoning from large language models. In this approach, the problem is broken down into multiple thought steps, and the model generates multiple thoughts per step. This results in a tree-like structure representing the model\\'s line of reasoning.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfNT72j2mZw-"
   },
   "source": [
    "What about the Cohere models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHye-j_MjYvL"
   },
   "outputs": [],
   "source": [
    "cohere_rag_chain = (\n",
    "    {\"context\": retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | cohere_chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "YnVEnMmNmrgM",
    "outputId": "95cd947d-7063-4ee7-d1d7-45bfa5f58f3d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Chain of Thought (CoT) is a prompting technique used to elicit reasoning and improve the performance of large language models on complex tasks. It involves decomposing a problem into multiple thought steps and generating a sequence of thoughts or reasoning chains to arrive at an answer. By providing intermediate steps and thoughts, CoT helps the language model to break down a complex task into simpler sub-tasks, enhancing its ability to provide justified and explainable responses.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_rag_chain.invoke('What is Chain of Thought?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtgVF-T8nN9o"
   },
   "source": [
    "Works too! Time to build the real thing and do experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM7gS9kGNOJp"
   },
   "source": [
    "## 3. The RAG Model & Experimentation\n",
    "\n",
    "With this we can get started. First, we need to acquire the data, chunk it, vectorize it, and store the embeddings (and in this simple case also the docs) in our Qdrant vector db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V21eJiV9GEg"
   },
   "source": [
    "### 3.1 The Vector Database\n",
    "\n",
    "We will start by creating our datastore, Qdrant. Usually, you would deploy the vector db as a server, but in this case let's simply put everything in memory. Also, in this case we will store not only the embeddings but the whole document in the vector store. We will seed the store with the splits from the blog post we had used before.\n",
    "\n",
    "We will also create the retriever, which defines the way the documents are being retrieved. The retriever parameters define for example which method is used, how many docs are retrieved, etc. See [this LangChain link ](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXxjbq6RsKq3"
   },
   "outputs": [],
   "source": [
    "qdrant_vectorstore = Qdrant.from_documents(splits,\n",
    "    base_embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"rag_tech_db\",\n",
    "    force_recreate=True\n",
    ")\n",
    "\n",
    "retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC9z0o5ZsLML"
   },
   "source": [
    "### 3.2 Data Acquisition, Chunking, and Vectorization\n",
    "\n",
    "Now where we have our store we need to get the data into it. We will need to retrieve the data, create the chunks, then vectorize them, and finally store the vectors (along with the docs in this case) in the vector db.\n",
    "\n",
    "Let us first set chunk size and overlap, as well as the type of splitter. These are starting parameters and you may want to experiment with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_BR4rgYrCGn"
   },
   "outputs": [],
   "source": [
    "#Note that these defaults may or may not be ideal!\n",
    "CHUNK_SIZE=128\n",
    "OVERLAP=0\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-vhTZ_tvIvD"
   },
   "source": [
    "Now let's work with an actual document collection.  We will work with four types of documents:\n",
    "\n",
    "* A few papers from the ArXiv on RAG and NLP\n",
    "* A few blogs from Lily Weng that talk about Open Domain Question Answering and related topics\n",
    "* A number of Wikipedia articles on that topic\n",
    "\n",
    "To make testing easier  we'll define a global record number so we can trace back to see which chunk came from which specific document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEmdCyqqx5kl"
   },
   "outputs": [],
   "source": [
    "#assign a unique number to each document we ingest\n",
    "global_doc_number = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_xFjAUSx7ES"
   },
   "source": [
    "First we'll grab some papers from ArXiv.  We'll grab the pdf files and get all of the pages as separate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6Jj2vLAyxic"
   },
   "outputs": [],
   "source": [
    "arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "                 '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "                 '2311.08377', '2312.05708', '2401.06532', '2401.17268', '2402.01306', '2402.19473', '2406.04744')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq8ETxzzygTj"
   },
   "outputs": [],
   "source": [
    "all_arxiv_pages = []\n",
    "\n",
    "#loop through the papers\n",
    "for identifier in arxiv_numbers:\n",
    "    # Construct URL using the arXiv unique identifier\n",
    "    arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "\n",
    "    # Extract pages from the document and add them to the list of pages\n",
    "    arx_loader = PyMuPDFLoader(arx_url)\n",
    "    arx_pages = arx_loader.load()\n",
    "    for page_num in range(len(arx_pages)):\n",
    "        page = arx_pages[page_num]\n",
    "        #CHANGED\n",
    "        page.metadata['page_num'] = page_num\n",
    "        page.metadata['doc_num'] = global_doc_number\n",
    "        page.metadata['doc_source'] = \"ArXiv\"\n",
    "        all_arxiv_pages.append(page)\n",
    "\n",
    "\n",
    "    global_doc_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYMkGJ7igvg_"
   },
   "source": [
    "How many docs did we get?  Is that the correct number? And what is the content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXNS5MaMwrOK",
    "outputId": "5459dfe6-6336-459f-88df-2e2c6e8ea7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 documents in total\n",
      "485 pages in total\n"
     ]
    }
   ],
   "source": [
    "num_pages = len(all_arxiv_pages)\n",
    "num_docs = global_doc_number - 1\n",
    "\n",
    "print(f\"{num_docs} documents in total\")\n",
    "print(f\"{num_pages} pages in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "f6Qr75rnvLKJ",
    "outputId": "97c88345-5125-4e7e-afb6-4367f21dc013"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arxiv_pages[5].page_content[:150]  # all pages of the Document content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czx-h_Aeg_NM"
   },
   "source": [
    "Now we need to split the docs into chunks.  LangChain provides a couple of ways to do that.  We'll use for now the `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ln3nmeVvLBI",
    "outputId": "4d8da7b9-3e8d-4ca9-a7d6-f658c271a03e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits/chunks:  17515\n"
     ]
    }
   ],
   "source": [
    "#index doc chunks\n",
    "splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "for idx, text in enumerate(splits):\n",
    "    splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVEpjCBLE50m",
    "outputId": "0d4e06ad-a366-4e2d-c37a-8ddb9362477e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/2005.11401.pdf', 'file_path': 'https://arxiv.org/pdf/2005.11401.pdf', 'page': 0, 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210413004838Z', 'modDate': 'D:20210413004838Z', 'trapped': '', 'page_num': 0, 'doc_num': 1, 'doc_source': 'ArXiv', 'split_id': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6DUxHWuozJB"
   },
   "source": [
    "Let's add the vectors to the datastore and see whether we can retrieve a nearest neighbor to a query. Let's look at the second closest match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSJmDkj6SvQQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "qdrant_vectorstore.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xOhcgKd5ckk"
   },
   "outputs": [],
   "source": [
    "query = \"How can we train a model for preferences?\"\n",
    "found_docs = qdrant_vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDelZRCF5Ite",
    "outputId": "b62863c1-8f45-4bac-8378-4952892c30f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One path forward could be to train models that can be conditioned on the preferences of certain\n",
      "0.8218969669738703\n"
     ]
    }
   ],
   "source": [
    "print(found_docs[0][0].page_content)\n",
    "print(found_docs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipT82FOghpr6"
   },
   "source": [
    "Next, let's get some information from Wikipedia on our main topic -- Gen AI.  LangChain provides a DocumentLoader that accesses the Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfWbJmg0vKfv",
    "outputId": "77bdd71a-3de1-466d-fa48-8e3a0a148cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  4\n",
      "Number of splits/chunks:  151\n"
     ]
    }
   ],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWD9eytoc7er"
   },
   "source": [
    "Now we'll add these splits to the vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC2LSdGwIcvn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsZSNcp4TWxO"
   },
   "source": [
    "Same with a couple of other queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlO0AYTmTaPM",
    "outputId": "ab402b3a-4057-4502-c843-724e9cc0368b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  4\n",
      "Number of splits/chunks:  160\n"
     ]
    }
   ],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQqX-w8tTjah"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtU-BeWpTkbv"
   },
   "source": [
    "And yet another related Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_uwyvZaTkx4",
    "outputId": "8877986e-12b6-4c8a-a3c2-9001f5eca719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  4\n",
      "Number of splits/chunks:  134\n"
     ]
    }
   ],
   "source": [
    "wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n",
    "for idx, text in enumerate(wiki_docs):\n",
    "    wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "    wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#index docs\n",
    "wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "for idx, text in enumerate(wiki_splits):\n",
    "    wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits/chunks: ', len(wiki_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw-FQnNZTk7I"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#vectorstore.add_documents(documents=wiki_splits, embedding=base_embeddings)\n",
    "qdrant_vectorstore.add_documents(documents=wiki_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfs8ro_Ziekd"
   },
   "source": [
    "We'll also augment our collection with some blog entries about Open Domain Question Answering, of which RAG is an approach, and some related topics in case users want to ask how the new Search system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5LdggJw8nBF",
    "outputId": "b804b8f7-a6c2-4d61-8caa-05a992b1640f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  5\n"
     ]
    }
   ],
   "source": [
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "               \"https://lilianweng.github.io/posts/2018-06-24-attention/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "               \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"),\n",
    "\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "web_documents = web_loader.load()\n",
    "\n",
    "for idx, text in enumerate(web_documents):\n",
    "    web_documents[idx].metadata['doc_num'] = global_doc_number\n",
    "    web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
    "global_doc_number += 1\n",
    "\n",
    "print('Number of documents: ', len(web_documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMjLDnHf8nBG"
   },
   "source": [
    "Again, we will split the retrieved data into chunks and add the data to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D29aVbKn8nBH",
    "outputId": "3a3d8aa8-6993-421b-cab0-fee6df3e06e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits:  2103\n"
     ]
    }
   ],
   "source": [
    "web_splits = text_splitter.split_documents(web_documents)\n",
    "\n",
    "for idx, text in enumerate(web_splits):\n",
    "    web_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "print('Number of splits: ', len(web_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5slq33qt6Dc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "qdrant_vectorstore.add_documents(documents=web_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoudXWjJB_p1"
   },
   "outputs": [],
   "source": [
    "retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KVUbsDa3BBF"
   },
   "source": [
    "### 3.3 The Test Data\n",
    "\n",
    "You will want to test the system that you (will) have built. Below we give you a validation set that you could take as labeled data (imagine, your user personas would have had these questions and deemed the answers to be good). We also will give you a test set that only contains questions. (This is the set that we will use to get a feel for how well your RAG system corresponds to our Gold model).\n",
    "\n",
    "Here are is the gold validation set and the test questions. **DO NOT CHANGE OR DELETE!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxMnyWig7R-T"
   },
   "outputs": [],
   "source": [
    "validation_questions_answers = {\n",
    "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
    "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"},\n",
    "1: {\"question\": \"How does a large language model learn from text during training?\",\n",
    "  \"gold_answer_research\": \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\",\n",
    "  \"gold_answer_marketing\": \"A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\"},\n",
    "2: {\"question\": \"What are some key architectures behind the development of large language models?\",\n",
    "  \"gold_answer_research\": \"Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\",\n",
    "  \"gold_answer_marketing\": \"Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\"},\n",
    "3: {\"question\": \"Can you name some specific large language models and the companies or organizations that have developed them?\",\n",
    "  \"gold_answer_research\": \"Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla by DeepMind, GPT-3 by OpenAI.\"},\n",
    "7: {\"question\": \"What licensing models have been adopted for the distribution of source-available language models?\",\n",
    "  \"gold_answer_research\": \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\"},\n",
    "8: {\"question\": \"What are language models and what is their purpose in natural language processing?\",\n",
    "  \"gold_answer_research\": \"Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\",\n",
    "  \"gold_answer_marketing\": \"Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\"},\n",
    "9: {\"question\": \"How have language models evolved in terms of architecture, from the 1980s to present times?\",\n",
    "  \"gold_answer_research\": \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\",\n",
    "  \"gold_answer_marketing\": \"Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\"},\n",
    "11: {\"question\": \"Can you explain how maximum entropy language models work and what the partition function signifies?\",\n",
    "  \"gold_answer_research\": \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\",\n",
    "  \"gold_answer_marketing\": \"Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\"},\n",
    "12: {\"question\": \"What is the benefit of using continuous space embeddings in recurrent neural network language models?\",\n",
    "  \"gold_answer_research\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\",\n",
    "  \"gold_answer_marketing\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\"},\n",
    "13: {\"question\": \"What challenges do large language models face in mirroring human cognitive patterns?\",\n",
    "  \"gold_answer_research\": \"Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\",\n",
    "  \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\"},\n",
    "16: {\"question\": \"What factors influenced the development of generative language models by Anthropic?\",\n",
    "  \"gold_answer_research\": \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\",\n",
    "  \"gold_answer_marketing\": \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"},\n",
    "17: {\"question\": \"What is Constitutional AI and how does it affect the functionality of AI systems?\",\n",
    "  \"gold_answer_research\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\",\n",
    "  \"gold_answer_marketing\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"},\n",
    "18: {\"question\": \"How do advances in AI models impact their ability to interact with different types of data, such as images?\",\n",
    "  \"gold_answer_research\": \"Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\",\n",
    "  \"gold_answer_marketing\": \"Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\"},\n",
    "19: {\"question\": \"What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\",\n",
    "  \"gold_answer_research\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\"},\n",
    "20: {\"question\": \"How has the token handling capacity changed between different versions of the Claude model?\",\n",
    "  \"gold_answer_research\": \"The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\",\n",
    "  \"gold_answer_marketing\": \"The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\"},\n",
    "22: {\"question\": \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\",\n",
    "  \"gold_answer_research\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\",\n",
    "  \"gold_answer_marketing\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"},\n",
    "23: {\"question\": \"How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\",\n",
    "  \"gold_answer_research\": \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\"},\n",
    "24: {\"question\": \"Who developed the language model family known as Chinchilla?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n",
    "  \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"},\n",
    "25: {\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n",
    "  \"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"},\n",
    "27: {\"question\": \"What is the relationship between Chinchilla and the Gopher language model families?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\"},\n",
    "28: {\"question\": \"What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\",\n",
    "  \"gold_answer_research\": \"The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\"},\n",
    "30: {\"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"},\n",
    "33: {\"question\": \"What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\",\n",
    "  \"gold_answer_research\": \"Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\",\n",
    "  \"gold_answer_marketing\": \"Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\"},\n",
    "34: {\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n",
    "  \"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n",
    "  \"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"},\n",
    "35: {\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n",
    "  \"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n",
    "  \"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"},\n",
    "36: {\"question\": \"What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\",\n",
    "  \"gold_answer_research\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\"},\n",
    "38: {\"question\": \"What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\",\n",
    "  \"gold_answer_research\": \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\",\n",
    "  \"gold_answer_marketing\": \"The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\"},\n",
    "39: {\"question\": \"What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\",\n",
    "  \"gold_answer_research\": \"Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\",\n",
    "  \"gold_answer_marketing\": \"Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\"},\n",
    "41: {\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n",
    "  \"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"},\n",
    "43: {\"question\": \"What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\",\n",
    "  \"gold_answer_research\": \"The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\",\n",
    "  \"gold_answer_marketing\": \"The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\"},\n",
    "44: {\"question\": \"What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\",\n",
    "  \"gold_answer_research\": \"Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\",\n",
    "  \"gold_answer_marketing\": \"The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\"},\n",
    "45: {\"question\": \"What are some challenges in training retrieval systems and how are negative samples used to address them?\",\n",
    "  \"gold_answer_research\": \"Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\",\n",
    "  \"gold_answer_marketing\": \"Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\"},\n",
    "46: {\"question\": \"What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\",\n",
    "  \"gold_answer_research\": \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\",\n",
    "  \"gold_answer_marketing\": \"Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\"},\n",
    "47: {\"question\": \"What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\",\n",
    "  \"gold_answer_research\": \"Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\",\n",
    "  \"gold_answer_marketing\": \"Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\"},\n",
    "48: {\"question\": \"What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\",\n",
    "  \"gold_answer_research\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\",\n",
    "  \"gold_answer_marketing\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\"},\n",
    "50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
    "  \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
    "  \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"},\n",
    "51: {\"question\": \"Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\",\n",
    "  \"gold_answer_research\": \"Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\",\n",
    "  \"gold_answer_marketing\": \"The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\"},\n",
    "52: {\"question\": \"What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\",\n",
    "  \"gold_answer_research\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\",\n",
    "  \"gold_answer_marketing\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\"},\n",
    "54: {\"question\": \"What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\",\n",
    "  \"gold_answer_research\": \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\",\n",
    "  \"gold_answer_marketing\": \"Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\"},\n",
    "55: {\"question\": \"What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\",\n",
    "  \"gold_answer_research\": \"The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\",\n",
    "  \"gold_answer_marketing\": \"The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\"},\n",
    "59: {\"question\": \"Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\",\n",
    "  \"gold_answer_research\": \"To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\"},\n",
    "60: {\"question\": \"What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\",\n",
    "  \"gold_answer_research\": \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\",\n",
    "  \"gold_answer_marketing\": \"Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\"},\n",
    "61: {\"question\": \"What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\",\n",
    "  \"gold_answer_research\": \"One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\",\n",
    "  \"gold_answer_marketing\": \"Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\"},\n",
    "62: {\"question\": \"What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\",\n",
    "  \"gold_answer_research\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\",\n",
    "  \"gold_answer_marketing\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\"},\n",
    "63: {\"question\": \"What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\",\n",
    "  \"gold_answer_research\": \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\",\n",
    "  \"gold_answer_marketing\": \"The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\"},\n",
    "64: {\"question\": \"What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\",\n",
    "  \"gold_answer_research\": \"In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\",\n",
    "  \"gold_answer_marketing\": \"The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\"},\n",
    "65: {\"question\": \"What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\",\n",
    "  \"gold_answer_research\": \"Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\",\n",
    "  \"gold_answer_marketing\": \"Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\"},\n",
    "67: {\"question\": \"What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\",\n",
    "  \"gold_answer_research\": \"Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\",\n",
    "  \"gold_answer_marketing\": \"The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\"},\n",
    "69: {\"question\": \"What is the role of manual assessment in the validation of language model predictions according to the text provided?\",\n",
    "  \"gold_answer_research\": \"Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\"},\n",
    "70: {\"question\": \"What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\",\n",
    "  \"gold_answer_research\": \"The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\",\n",
    "  \"gold_answer_marketing\": \"The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\"},\n",
    "73: {\"question\": \"What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\",\n",
    "  \"gold_answer_research\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\",\n",
    "  \"gold_answer_marketing\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\"},\n",
    "74: {\"question\": \"What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\",\n",
    "  \"gold_answer_research\": \"Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\",\n",
    "  \"gold_answer_marketing\": \"Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\"},\n",
    "75: {\"question\": \"Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\",\n",
    "  \"gold_answer_research\": \"Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\",\n",
    "  \"gold_answer_marketing\": \"Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\"},\n",
    "76: {\"question\": \"What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\",\n",
    "  \"gold_answer_research\": \"A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\",\n",
    "  \"gold_answer_marketing\": \"A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\"},\n",
    "78: {\"question\": \"What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\",\n",
    "  \"gold_answer_research\": \"According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\",\n",
    "  \"gold_answer_marketing\": \"The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\"},\n",
    "80: {\"question\": \"How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\",\n",
    "  \"gold_answer_research\": \"Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\",\n",
    "  \"gold_answer_marketing\": \"KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\"},\n",
    "81: {\"question\": \"What are some common approaches to building an open-domain question answering system?\",\n",
    "  \"gold_answer_research\": \"Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\",\n",
    "  \"gold_answer_marketing\": \"Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\"},\n",
    "82: {\"question\": \"What is the difference between open-book and closed-book question answering?\",\n",
    "  \"gold_answer_research\": \"Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\",\n",
    "  \"gold_answer_marketing\": \"Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\"},\n",
    "84: {\"question\": \"What are the basic components of the Retriever-Reader framework in open-domain QA?\",\n",
    "  \"gold_answer_research\": \"The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\",\n",
    "  \"gold_answer_marketing\": \"The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\"},\n",
    "85: {\"question\": \"How is the TF-IDF model used in question answering retrieval systems?\",\n",
    "  \"gold_answer_research\": \"In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\"},\n",
    "86: {\"question\": \"Can neural networks enhance the process of information retrieval in QA systems?\",\n",
    "  \"gold_answer_research\": \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\",\n",
    "  \"gold_answer_marketing\": \"Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\"},\n",
    "87: {\"question\": \"What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\",\n",
    "  \"gold_answer_marketing\": \"Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\"},\n",
    "88: {\"question\": \"How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\",\n",
    "  \"gold_answer_marketing\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\"},\n",
    "89: {\"question\": \"What is the main goal of prompt engineering in language models?\",\n",
    "  \"gold_answer_research\": \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n",
    "  \"gold_answer_marketing\": \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\"},\n",
    "91: {\"question\": \"What are some known biases that can affect the performance of few-shot classification in LLMs?\",\n",
    "  \"gold_answer_research\": \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\",\n",
    "  \"gold_answer_marketing\": \"Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\"},\n",
    "92: {\"question\": \"Why might increasing model size not reduce variance in model performance with varying prompts?\",\n",
    "  \"gold_answer_research\": \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\",\n",
    "  \"gold_answer_marketing\": \"Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\"},\n",
    "93: {\"question\": \"What is the benefit of instruction-based finetuning in language models?\",\n",
    "  \"gold_answer_research\": \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\",\n",
    "  \"gold_answer_marketing\": \"The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\"},\n",
    "94: {\"question\": \"Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\",\n",
    "  \"gold_answer_research\": \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\",\n",
    "  \"gold_answer_marketing\": \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\"},\n",
    "95: {\"question\": \"What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\",\n",
    "  \"gold_answer_research\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\",\n",
    "  \"gold_answer_marketing\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\"},\n",
    "96: {\"question\": \"How do augmented language models with external tools differ from regular models in functionality?\",\n",
    "  \"gold_answer_research\": \"Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\",\n",
    "  \"gold_answer_marketing\": \"Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\"},\n",
    "97: {\"question\": \"What can be inferred about the utilization of attention in neural networks?\",\n",
    "  \"gold_answer_research\": \"Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\",\n",
    "  \"gold_answer_marketing\": \"Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\"},\n",
    "101: {\"question\": \"Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\",\n",
    "  \"gold_answer_research\": \"Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\",\n",
    "  \"gold_answer_marketing\": \"Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\"},\n",
    "102: {\"question\": \"What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\",\n",
    "  \"gold_answer_research\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\",\n",
    "  \"gold_answer_marketing\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\"},\n",
    "103: {\"question\": \"How does the transformer model variate from traditional sequence-aligned recurrent architectures?\",\n",
    "  \"gold_answer_research\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\"},\n",
    "104: {\"question\": \"What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\",\n",
    "  \"gold_answer_research\": \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\",\n",
    "  \"gold_answer_marketing\": \"The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\"},\n",
    "}\n",
    "\n",
    "\n",
    "test_questions = {\n",
    "4: {\"question\": \"When was the transformer architecture introduced, and by which organization?\"},\n",
    "5: {\"question\": \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"},\n",
    "6: {\"question\": \"What benchmarks or ratings are used to compare the capabilities of different language models?\"},\n",
    "10: {\"question\": \"What are some of the primary applications for language models in technology and computing?\"},\n",
    "14: {\"question\": \"How are language models typically evaluated and what benchmarks are used for this purpose?\"},\n",
    "15: {\"question\": \"What datasets are available for evaluating language processing systems?\"},\n",
    "21: {\"question\": \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"},\n",
    "26: {\"question\": \"According to DeepMind, how should the number of training tokens change relative to the model size?\"},\n",
    "29: {\"question\": \"How do the sizes of models in the Gopher family range?\"},\n",
    "31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
    "32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
    "37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
    "40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
    "42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
    "49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
    "56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
    "57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
    "58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
    "66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
    "68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
    "71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
    "72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
    "77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
    "79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
    "83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
    "90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
    "98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
    "99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
    "100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwAWpJTqzykx"
   },
   "outputs": [],
   "source": [
    "eng_ans = [inner_dict['gold_answer_research'] for inner_dict in validation_questions_answers.values() if 'gold_answer_research' in inner_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BVNjxat1Hmz"
   },
   "outputs": [],
   "source": [
    "mkt_ans = [inner_dict['gold_answer_marketing'] for inner_dict in validation_questions_answers.values() if 'gold_answer_marketing' in inner_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj1ZqUVZ0m-s",
    "outputId": "0761fd36-e11f-43ff-a309-cb0bd4dbd31b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(pd.Series(eng_ans).apply(lambda x: len(x.split('.'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stjxD0dQ1NTS",
    "outputId": "b70bc9aa-ef0e-4a76-b714-87c36fdc9d6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.493333333333333"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pd.Series(mkt_ans).apply(lambda x: len(x.split('.'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC86ggPEuipg"
   },
   "source": [
    "### 3.3 Running the RAG System\n",
    "\n",
    "Let's have a quick look at the validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABdr23NUSbTj",
    "outputId": "7fb3a36e-7f6e-4f98-8f5d-0f480ab80dbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What purpose do large language models serve in the field of natural language processing?',\n",
       " 'gold_answer_research': 'Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.',\n",
       " 'gold_answer_marketing': 'Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_questions_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ra_p5DhzSbeN",
    "outputId": "c9474129-043f-470f-f856-029c10e327dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When was the transformer architecture introduced, and by which organization?'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FQbNGECSq1g"
   },
   "source": [
    "Let's now use the data to ask questions against it. So we need to define our prompt templates, the RAG Chain, etc.\n",
    "\n",
    "We have two types of User Personas we need to support:\n",
    "\n",
    "1. The engineers, who require pretty detailed information when they ask questions  \n",
    "2. The marketing team and supporting staff who also will ask questions around GenAI in order to better understand the products and the field as a whole, but a lot more high level answers would likely be in order\n",
    "\n",
    "**Below, please build your RAG pipeline including the relevant prompts. This is free form so you will need to create your own cells, text documentation as you need, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDvwjtRlJ0fC"
   },
   "source": [
    "#### 3.3.0 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvKn6Q0oNPKv"
   },
   "outputs": [],
   "source": [
    "# creating a custom output parser\n",
    "class CustomOutputParser:\n",
    "    def __call__(self, output):\n",
    "        # Define a regular expression pattern to match the \"Assistant:\" line\n",
    "        pattern = r\"Assistant:\\s*(.*)\"\n",
    "\n",
    "        # Search for the pattern in the output\n",
    "        match = re.search(pattern, output)\n",
    "\n",
    "        if match:\n",
    "            # Return the captured group (the actual answer)\n",
    "            return match.group(1).strip()\n",
    "\n",
    "        # If no match is found, return None\n",
    "        return None\n",
    "\n",
    "custom_output_parser = CustomOutputParser()\n",
    "\n",
    "# creating a document formatter\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK0vthVnJbIB"
   },
   "source": [
    "#### 3.3.1 LLM Configurations with Temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQOhCwHGdTaE"
   },
   "source": [
    "##### Functions to generate LLMs given hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRInuHWBa24c"
   },
   "outputs": [],
   "source": [
    "# function to build a cohere chat model\n",
    "def build_cohere_llm_lc():\n",
    "    COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
    "    cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)\n",
    "    return cohere_chat_model\n",
    "\n",
    "# function to build a mistral LLM in the LangChain environment\n",
    "def build_mistral_llm_lc(max_new_tokens = 500,temperature = 0.6,top_p = 0.95,do_sample = True,repetition_penalty = 1.2):\n",
    "  quantization_config = BitsAndBytesConfig(load_in_4bit=True, )\n",
    "  llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "      \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "      torch_dtype=torch.float32,\n",
    "      device_map='auto',\n",
    "      quantization_config=quantization_config)\n",
    "  llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "  mistral_pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model=llm_mistral_model, # specifying the pre-trained model to use for text generation\n",
    "      tokenizer=llm_mistral_tokenizer, # specifying the tokenizer\n",
    "      max_new_tokens=max_new_tokens, # sets the maximum number of new tokens to generate.\n",
    "      temperature=temperature, # controls the randomness of predictions\n",
    "      top_p=top_p, # generates more coherent text\n",
    "      do_sample=do_sample, # enables sampling\n",
    "      repetition_penalty=repetition_penalty # penalizes repeated tokens to avoid redundancy in the generated text.\n",
    "  )\n",
    "  # setting the padding token ID\n",
    "  mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n",
    "  mistral_llm_lc = HuggingFacePipeline(pipeline=mistral_pipe)\n",
    "  return mistral_llm_lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpZ5NkJsJjBR"
   },
   "source": [
    "##### Mistral with temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy3iedsPKNYu"
   },
   "outputs": [],
   "source": [
    "# naming a variable for the mistral llm with temperature 0.2\n",
    "mistral_llm1_lc = build_mistral_llm_lc(temperature = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh2RLMnlJlwX"
   },
   "source": [
    "##### Mistral with temperature 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN_n2G-BKN6m"
   },
   "outputs": [],
   "source": [
    "# naming a variable for the mistral llm with temperature 0.6\n",
    "mistral_llm2_lc = build_mistral_llm_lc(temperature = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svmFY3xhy6I1"
   },
   "source": [
    "##### Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6HqOkQye4jo"
   },
   "outputs": [],
   "source": [
    "# naming a variable for the cohere chat model\n",
    "cohere_llm_lc = build_cohere_llm_lc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9yrCoJvHIXl"
   },
   "source": [
    "##### Function to get the LLM given the LLM model name and the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJkhnhA-xWMs"
   },
   "outputs": [],
   "source": [
    "# function to get the LLM variable given the model name and temperature\n",
    "def get_llm(llm_type, temperature):\n",
    "  if llm_type == 'mistral':\n",
    "    if temperature == 0.2:\n",
    "      return mistral_llm1_lc\n",
    "    if temperature == 0.6:\n",
    "      return mistral_llm2_lc\n",
    "  if llm_type == 'cohere':\n",
    "      # for cohere temperature can be None\n",
    "      return cohere_llm_lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxHQOiAeJFOC"
   },
   "source": [
    "#### 3.3.2 Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFWLLPMgJP0x"
   },
   "source": [
    "##### Prompt A_eng and Prompt A_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXLdaQAYKXCM"
   },
   "outputs": [],
   "source": [
    "# writing the set of version A prompts\n",
    "prompt_a_eng = \"\"\"[INST]\n",
    "You are a helpful question answering assistant specializing in NLP and GenAI. Your audience is a team of research engineers at a tech company that wants to roll out a new series of GenAI products.\n",
    "These engineers understand NLP on a technical level, so they will need answers that provide appropriate technical depth and detail.\n",
    "Please answer the engineers' questions using only the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "That is it for the context.\n",
    "\n",
    "Now, you will answer the following question using the context provided in less than 100 words. Remember, you are answering the question for engineers who collectively have a good understanding of NLP already and require detailed and comprehensive answers. While your response should be comprehensive, please use the context provided above as your main source of information and keep it as concise as appropriate.\n",
    "\n",
    "Answer the question directly in appropriate detail without any lists or bullet points.\n",
    "\n",
    "Here is the question:\n",
    "\n",
    "{question}\n",
    "[/INST]\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_a_mkt = \"\"\"[INST]\n",
    "You are a helpful question answering assistant specializing in NLP and GenAI. Your audience is a marketing team at a tech company that wants to roll out a new series of GenAI products.\n",
    "This marketing team does not understand NLP on a technical level, so they will not understand answers with overly technical terminology.\n",
    "Please answer the marketing team's questions using only the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "That is it for the context.\n",
    "\n",
    "Now, you will answer the following question using the context provided in less than 75 words. Remember, you are answering the question for a marketing team that collectively does not have a deep understanding of NLP. The purpose of your answer will be to provide the marketing team with high level information. Your answers will help the marketing team better understand their tech company's GenAI products and the GenAI field as a whole. While your response should be high level, please use the context provided above as your main source of information and keep it as concise as appropriate.\n",
    "\n",
    "Answer the question directly in appropriate detail without any lists or bullet points.\n",
    "\n",
    "Here is the question:\n",
    "\n",
    "{question}\n",
    "[/INST]\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UPAAJqtJVhV"
   },
   "source": [
    "##### Prompt B_eng and Prompt B_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7axX7E2TKXi3"
   },
   "outputs": [],
   "source": [
    "# writing the set of version B prompts\n",
    "prompt_b_eng = \"\"\"[INST]\n",
    "You are a highly knowledgeable question-answering assistant specializing in the fields of Natural Language Processing (NLP) and Generative AI (GenAI). Your audience consists of a team of research engineers at a tech company preparing to launch a new series of GenAI products. These engineers have a strong technical background in NLP, so your responses must provide appropriate technical depth and detail.\n",
    "\n",
    "Please answer the engineers' questions, only using the following context to inform your answer:\n",
    "\n",
    "{context}\n",
    "\n",
    "That is the context.\n",
    "\n",
    "Now, answer the following question in less than 100 words, ensuring your response is detailed, comprehensive, and technically robust, tailored to engineers with a solid understanding of NLP.\n",
    "\n",
    "Your response should use the context provided above as its main source of information. Answer the question directly, without any prefatory or introductory phrases. Provide a comprehensive, continuous answer, that utilizes the context provided above, without resorting to lists or bullet points. Again, keep your answer within the ~100 word limit and respond in paragraph form.\n",
    "\n",
    "Here is the question:\n",
    "\n",
    "{question}\n",
    "[/INST]\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_b_mkt = \"\"\"[INST]\n",
    "You are a highly knowledgeable question-answering assistant specializing in Natural Language Processing (NLP) and Generative AI (GenAI). Your audience is a marketing team at a tech company preparing to launch a new series of GenAI products. This marketing team does not have a deep technical understanding of NLP, so avoid using overly technical terminology, and avoid going into highly technical depth and detail.\n",
    "\n",
    "Please answer the marketing team's questions, only using the following context to inform your answer:\n",
    "\n",
    "{context}\n",
    "\n",
    "That is the context.\n",
    "\n",
    "Now, answer the following question in less than 75 words, ensuring your response is high-level enough to be understood by a non-technical audience. Your goal is to help them better understand their company's GenAI products and the GenAI field in general. Your response should use the context provided above as its main source of information.\n",
    "\n",
    "Your response should use the context provided above as its main source of information. Answer the question directly, without any prefatory or introductory phrases. Provide a comprehensive, continuous answer, that utilizes the context provided above, without resorting to lists or bullet points. Again, keep your answer within the ~75 word limit and respond in paragraph form.\n",
    "\n",
    "Here is the question:\n",
    "\n",
    "{question}\n",
    "[/INST]\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSYg7NZVijdG"
   },
   "source": [
    "##### Prompt C_eng and Prompt C_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LxhDeIeisr1"
   },
   "outputs": [],
   "source": [
    "# writing the set of version C prompts\n",
    "prompt_c_eng = \"\"\"[INST]\n",
    "You are a question answering assistant specializing in Natural Language Processing (NLP) and Generative AI (GenAI). Your job is to assist a team of research engineers at a tech company that is launching a new series of GenAI products.\n",
    "These engineers understand NLP on a technical level, so they will need answers that provide appropriate technical depth and detail. You may want to use the following context when generating your response. Here is the context:\n",
    "{context}\n",
    "Now that you have some context, you will answer the following question in less than 100 words in a single paragraph format.\n",
    "Do not use prefatory or introductory clauses. Do not use bullet points or numbered lists.\n",
    "Just provide around 3 to 5 sentences, answering the question directly.\n",
    "Remember, you are answering the question for engineers who collectively have a good understanding of NLP and require answers with technical detail. While your response should be comprehensive, please keep your answer appropriately concise and relevant to the question.\n",
    "Here is the question:\n",
    "{question}\n",
    "[/INST]\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_c_mkt = \"\"\"[INST]\n",
    "You are a question answering assistant specializing in Natural Language Processing (NLP) and Generative AI (GenAI). Your job is to assist a marketing team at a tech company that is launching a new series of GenAI products.\n",
    "This marketing team does not understand NLP on a technical level, so they will need answers that provide an appropriate level of high-level detail. You may want to use the following context when generating your response. Here is the context:\n",
    "{context}\n",
    "Now that you have some context, you will answer the following question in less than 75 words in a single paragraph format.\n",
    "Do not use prefatory or introductory clauses. Do not use bullet points or numbered lists.\n",
    "Just provide around 1 to 3 sentences, answering the question directly.\n",
    "Remember, you are answering the question for a marketing team that collectively does not have a deeply technical understanding of NLP and requires high-level answers with only appropriate technical detail. While your response should be high-level and concise, please keep your answer appropriately detailed and relevant to the question.\n",
    "Here is the question:\n",
    "{question}\n",
    "[/INST]\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwkwTigJHkDg"
   },
   "source": [
    "##### Function to get the RAG prompt given the prompt version letter and the 3 letter indicator for audience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W3HkbHJtoit"
   },
   "outputs": [],
   "source": [
    "# function to get the RAG prompt given the prompt version letter and the 3 letter indicator for audience\n",
    "def get_rag_prompt(prompt, audience):\n",
    "  if prompt=='A' and audience=='eng':\n",
    "    rag_prompt = prompt_a_eng\n",
    "    return rag_prompt\n",
    "  if prompt=='A' and audience=='mkt':\n",
    "    rag_prompt = prompt_a_mkt\n",
    "    return rag_prompt\n",
    "  if prompt=='B' and audience=='eng':\n",
    "    rag_prompt = prompt_b_eng\n",
    "    return rag_prompt\n",
    "  if prompt=='B' and audience=='mkt':\n",
    "    rag_prompt = prompt_b_mkt\n",
    "    return rag_prompt\n",
    "  if prompt=='C' and audience=='eng':\n",
    "    return prompt_c_eng\n",
    "  if prompt=='C' and audience=='mkt':\n",
    "    return prompt_c_mkt\n",
    "  else:\n",
    "    print('prompt must be one of A or B or C and audience must be one of eng or mkt.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNUOWYENU0RB"
   },
   "source": [
    "#### 3.3.3 Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3BowqgfH0in"
   },
   "source": [
    "##### Setting the variable for the \"multi-qa-mpnet-base-dot-v1\" embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lfeSc3t7Guv"
   },
   "outputs": [],
   "source": [
    "# setting the variable for the \"multi-qa-mpnet-base-dot-v1\" embedding model\n",
    "base_embeddings1 = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YE0BgLKIA5t"
   },
   "source": [
    "##### Setting the variable for the \"all-distilroberta-v1\" embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J_gXpyuU9Zy"
   },
   "outputs": [],
   "source": [
    "# setting the variable for the \"all-distilroberta-v1\" embedding model\n",
    "base_embeddings2 = HuggingFaceEmbeddings(model_name=\"all-distilroberta-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKKhZaaBJpS4"
   },
   "source": [
    "#### 3.3.3 Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6-ddD8_NZ7e"
   },
   "source": [
    "##### Function to get Retriever given Chunk Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVVPUDKbNglF"
   },
   "outputs": [],
   "source": [
    "# function to get the retriever given the chunking parameters\n",
    "def build_retriever(chunk_size, overlap, base_embeddings):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    vectorstore = Qdrant.from_documents(splits,\n",
    "        base_embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"test2\",)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    qdrant_vectorstore = Qdrant.from_documents(splits,\n",
    "        base_embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"rag_tech_db2\",\n",
    "        force_recreate=True)\n",
    "    retriever = qdrant_vectorstore.as_retriever()\n",
    "\n",
    "    global_doc_number = 1\n",
    "\n",
    "    arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "                    '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "                    '2311.08377', '2312.05708', '2401.06532', '2401.17268', '2402.01306', '2402.19473', '2406.04744')\n",
    "    all_arxiv_pages = []\n",
    "    for identifier in arxiv_numbers:\n",
    "      arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "      arx_loader = PyMuPDFLoader(arx_url)\n",
    "      arx_pages = arx_loader.load()\n",
    "      for page_num in range(len(arx_pages)):\n",
    "          page = arx_pages[page_num]\n",
    "          page.metadata['page_num'] = page_num\n",
    "          page.metadata['doc_num'] = global_doc_number\n",
    "          page.metadata['doc_source'] = \"ArXiv\"\n",
    "          all_arxiv_pages.append(page)\n",
    "    splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "    for idx, text in enumerate(splits):\n",
    "        splits[idx].metadata['split_id'] = idx\n",
    "    qdrant_vectorstore.add_documents(documents=splits)\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "    wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
    "    for idx, text in enumerate(wiki_docs):\n",
    "        wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "        wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "    wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "    for idx, text in enumerate(wiki_splits):\n",
    "        wiki_splits[idx].metadata['split_id'] = idx\n",
    "    qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "    wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n",
    "    for idx, text in enumerate(wiki_docs):\n",
    "        wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "        wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "    wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "    for idx, text in enumerate(wiki_splits):\n",
    "        wiki_splits[idx].metadata['split_id'] = idx\n",
    "    qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "    wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n",
    "    for idx, text in enumerate(wiki_docs):\n",
    "        wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "        wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "    wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "    for idx, text in enumerate(wiki_splits):\n",
    "        wiki_splits[idx].metadata['split_id'] = idx\n",
    "    qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "    web_loader = WebBaseLoader(\n",
    "        web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "                  \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "                  \"https://lilianweng.github.io/posts/2018-06-24-attention/\",\n",
    "                  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                  \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"),\n",
    "        bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
    "    web_documents = web_loader.load()\n",
    "    for idx, text in enumerate(web_documents):\n",
    "        web_documents[idx].metadata['doc_num'] = global_doc_number\n",
    "        web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
    "    web_splits = text_splitter.split_documents(web_documents)\n",
    "    for idx, text in enumerate(web_splits):\n",
    "        web_splits[idx].metadata['split_id'] = idx\n",
    "    qdrant_vectorstore.add_documents(documents=web_splits)\n",
    "\n",
    "    retriever = qdrant_vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPdCw4uUJy2e"
   },
   "source": [
    "##### Chunk Size 128 with 0 overlap, with both embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbV-QWgIKZ-a"
   },
   "outputs": [],
   "source": [
    "# setting variables for the 128/0 chunking parameters for the first embedding model\n",
    "retriever1_1 = build_retriever(chunk_size=128, overlap=0, base_embeddings=base_embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLkIvnpRKakW"
   },
   "outputs": [],
   "source": [
    "# setting variables for the 128/0 chunking parameters for the second embedding model\n",
    "retriever1_2 = build_retriever(chunk_size=128, overlap=0, base_embeddings=base_embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6A2pY8eJ401"
   },
   "source": [
    "##### Chunk Size 300 with 40 overlap, with both embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNI_RYpBaYPU"
   },
   "outputs": [],
   "source": [
    "# setting variables for the 300/40 chunking parameters for the first embedding model\n",
    "retriever2_1 = build_retriever(chunk_size=300, overlap=40, base_embeddings=base_embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3It5kTkWjEh"
   },
   "outputs": [],
   "source": [
    "# setting variables for the 300/40 chunking parameters for the second embedding model\n",
    "retriever2_2 = build_retriever(chunk_size=300, overlap=40, base_embeddings=base_embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfn7W33WIQ09"
   },
   "source": [
    "##### Function to get the Retriever given the embedding model, chunk size, and chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAgo3MS0tGRV"
   },
   "outputs": [],
   "source": [
    "# function to get the retriever given the embedding model, chunk size, and chunk overlap\n",
    "def get_retriever(embedding_model, chunk_size, overlap):\n",
    "  if embedding_model=='multi-qa-mpnet-base-dot-v1' and chunk_size==128 and overlap==0:\n",
    "    retriever = retriever1_1\n",
    "    return retriever\n",
    "  if embedding_model=='multi-qa-mpnet-base-dot-v1' and chunk_size==300 and overlap==40:\n",
    "    retriever = retriever2_1\n",
    "    return retriever\n",
    "  if embedding_model=='all-distilroberta-v1' and chunk_size==128 and overlap==0:\n",
    "    retriever = retriever1_2\n",
    "    return retriever\n",
    "  if embedding_model=='all-distilroberta-v1' and chunk_size==300 and overlap==40:\n",
    "    retriever = retriever2_2\n",
    "    return retriever\n",
    "  else:\n",
    "    print('embedding_model must be one of multi-qa-mpnet-base-dot-v1 or all-distilroberta-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0Zsv5-POa76"
   },
   "source": [
    "#### 3.3.4 RAG Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "455wGUEqWMLA"
   },
   "source": [
    "##### Functions to build the RAG chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSHUH4nlWPiH"
   },
   "outputs": [],
   "source": [
    "# function to build the mistral rag chain given the retriever, prompt, and LLM\n",
    "def build_rag_chain_mistral(retriever, rag_prompt, llm_lc):\n",
    "    rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
    "    rag_chain = (\n",
    "      {\"context\": retriever | format_docs,\n",
    "      \"question\": RunnablePassthrough()}\n",
    "      | rag_prompt_template\n",
    "      | llm_lc\n",
    "      | custom_output_parser\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "# function to build the cohere rag chain given the retriever, prompt, and LLM\n",
    "def build_rag_chain_cohere(retriever, rag_prompt, llm_lc):\n",
    "    output_parser = StrOutputParser()\n",
    "    rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
    "    test_cohere_llm_chain_short = (\n",
    "       {\"context\": retriever | format_docs,\n",
    "       \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt_template\n",
    "        | llm_lc\n",
    "        | output_parser\n",
    "       )\n",
    "    return test_cohere_llm_chain_short\n",
    "\n",
    "# function to build the rag chain given the model name, embedding model name, prompt version letter, audience 3-letter indicator, temperature, chunk_size, and overlap\n",
    "def build_rag_chain(model='mistral', embedding_model='multi-qa-mpnet-base-dot-v1', prompt='A', audience='eng', temperature=0.2, chunk_size=128, overlap=0):\n",
    "  retriever = get_retriever(embedding_model, chunk_size, overlap)\n",
    "  rag_prompt = get_rag_prompt(prompt, audience)\n",
    "  llm_lc = get_llm(model, temperature)\n",
    "\n",
    "  if model=='cohere':\n",
    "    chain=build_rag_chain_cohere(retriever, rag_prompt, llm_lc)\n",
    "  if model=='mistral':\n",
    "    llm_lc= build_mistral_llm_lc(temperature=temperature)\n",
    "    chain=build_rag_chain_mistral(retriever, rag_prompt, llm_lc)\n",
    "  return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJE1wexvV7ay"
   },
   "source": [
    "##4. Tests & Evaluations\n",
    "\n",
    "Here you should evaluate the results. First, you should define your evaluation metrics and then you should run evaluation tests. This is really your area, but key results to show are:\n",
    "\n",
    "1) Your metrics of choice  \n",
    "2) How  your various models compare to the labeled validation data.\n",
    "\n",
    "Make sure you look at the results for the marketing team and the research team separately.\n",
    "\n",
    "**Note:** You do not need to run all models against all labeled questions, as that may take some time. Just do that for a few models/configs, and test a larger set with a smaller subset.\n",
    "\n",
    "**This is free form so you will need to create your own cells, text documentation as you need, etc.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOoTy9lG9BxI"
   },
   "source": [
    "### 4.1. Metrics\n",
    "\n",
    "Please define and motivate your metrics here. Please feel free to add more text and code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKeFvA6ovIIU"
   },
   "source": [
    "1. The first metric I will be using is simple cosine similarty\n",
    "* This metric will measure the overall similarity between the RAG answer and the gold standard answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nca6pT4X5b7O"
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(gold_standard, llm_answer):\n",
    "  g_s_embedded = base_embeddings2.embed_query(gold_standard)\n",
    "  llm_a_embedded = base_embeddings2.embed_documents([llm_answer])\n",
    "  cos_sim = cosine_similarity([g_s_embedded], llm_a_embedded)[0][0]\n",
    "  return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36Gv4A3kvbDe"
   },
   "source": [
    "2. The second metric I will be using is the BERTScore metric\n",
    "* The BERTScore metric first creates embedding representations of the RAG answer and the gold standard answer\n",
    "* Then, it generates similarity scores between the embeddings using cosine similarity\n",
    "* Then, each token in the RAG generated sentence is matched with its most similar token in the gold standard answer\n",
    "* Then, rare word importance is considered using Inverse Document Frequency\n",
    "* Finally, the score is rescaled to an interpretable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsrHrlWAtDvX"
   },
   "outputs": [],
   "source": [
    "# BERTScore precision, recall, F1 score\n",
    "def get_bert_score(gold_standard, llm_answer):\n",
    "  scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "  P, R, F1 = scorer.score([llm_answer], [gold_standard])\n",
    "  return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPAZOBhOiK1P"
   },
   "outputs": [],
   "source": [
    "def create_eval_df(val_df, audience):\n",
    "  if audience == 'eng':\n",
    "    gold_col = 'gold_answer_research'\n",
    "    rag_col = 'rag_answer_research'\n",
    "  if audience == 'mkt':\n",
    "    gold_col = 'gold_answer_marketing'\n",
    "    rag_col = 'rag_answer_marketing'\n",
    "  val_df['cos_similarity'] = val_df.apply(lambda row: get_cosine_similarity(row[gold_col],row[rag_col]),axis=1)\n",
    "  val_df_bert = pd.json_normalize(val_df.apply(lambda row: get_bert_score(row[gold_col],row[rag_col]),axis=1))\n",
    "  val_df['bert_precision'] = val_df_bert['precision']\n",
    "  val_df['bert_recall'] = val_df_bert['recall']\n",
    "  val_df['bert_f1'] = val_df_bert['f1']\n",
    "  return val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqfBWaSwkf_4"
   },
   "outputs": [],
   "source": [
    "def create_overall_metrics_df(val_df_eval, pipeline_num, audience):\n",
    "  return pd.DataFrame({'pipeline': [pipeline_num],\n",
    "                       'audience': [audience],\n",
    "                       'avg_cos_similarity':[val_df_eval.cos_similarity.mean()],\n",
    "                       'avg_bert_precision':[val_df_eval.bert_precision.mean()],\n",
    "                       'avg_bert_recall':[val_df_eval.bert_recall.mean()],\n",
    "                       'avg_bert_f1':[val_df_eval.bert_f1.mean()],\n",
    "                       'combined_score':[(val_df_eval.cos_similarity.mean() + val_df_eval.bert_f1.mean())/2]\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKktTilyTy_z"
   },
   "source": [
    "### 4.2. Evaluation Comparisons\n",
    "\n",
    "Document your key runs here. Feel free to add more text and code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjsLfy17LJr8"
   },
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(validation_questions_answers).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZfPA8LP9UOM"
   },
   "source": [
    "#### 4.2.2 RAG pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65tnK2Amkop3",
    "outputId": "95a00de0-5214-4386-ae28-5cc0afc2be57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvXhgyo4kzAv"
   },
   "source": [
    "##### Pipeline 1\n",
    "* model='Cohere'\n",
    "* embedding_model='multi-qa-mpnet-base-dot-v1'\n",
    "* Prompt A\n",
    "* chunk_size=128, overlap=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRlFCN1OlN0N"
   },
   "outputs": [],
   "source": [
    "# build rag given configuration\n",
    "rag_pipe_1_eng = build_rag_chain(model='cohere',\n",
    "                embedding_model='multi-qa-mpnet-base-dot-v1',\n",
    "                prompt='A',\n",
    "                audience='eng',\n",
    "                temperature=None,\n",
    "                chunk_size=128,\n",
    "                overlap=0)\n",
    "\n",
    "rag_pipe_1_mkt = build_rag_chain(model='cohere',\n",
    "                embedding_model='multi-qa-mpnet-base-dot-v1',\n",
    "                prompt='A',\n",
    "                audience='mkt',\n",
    "                temperature=None,\n",
    "                chunk_size=128,\n",
    "                overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_DdYUt8pSZq"
   },
   "source": [
    "###### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxtQ_RP8omaC"
   },
   "outputs": [],
   "source": [
    "val_df1_eng = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOqsQUebeNL3"
   },
   "outputs": [],
   "source": [
    "rag1_answers_research = []\n",
    "for question in val_df1_eng.iloc[:10,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fu4w6JolJuD"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aN3oU-JZWv_"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89GZrR-eaPBY"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ogtl6gXJdGG8"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaBQSTcKAo_2"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA9NsU3UApSe"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xb44mpxAu6p"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_eng.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_1_eng.invoke(question)\n",
    "  rag1_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntbp95IKajVl"
   },
   "outputs": [],
   "source": [
    "val_df1_eng['rag_answer_research'] = rag1_answers_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mfexvILlZuw"
   },
   "outputs": [],
   "source": [
    "val_df1_eng.to_csv('/content/drive/My Drive/290/rag_answers_eng/rag1_answers_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwMm8T8phUg6"
   },
   "source": [
    "###### Evaluating Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxgt3ZOuhKh2"
   },
   "outputs": [],
   "source": [
    "val_df1_eng = pd.read_csv('/content/drive/My Drive/290/rag_answers_eng/rag1_answers_eng.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atFIhMPuEljN"
   },
   "outputs": [],
   "source": [
    "val_df1_eng_eval = create_eval_df(val_df1_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "UuPCGsKVFjbs",
    "outputId": "f462ed5a-aad4-46de-9254-fb080e3476d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df1_eng_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"The distribution and usage rights of source-available language models are governed by various licensing models, which are essential for determining how these models can be utilized and shared by different entities. The MIT license, a permissive license, is commonly adopted for source-available language models. This license grants users the permission to use, copy, modify, merge, publish, distribute, sublicense, and sell the copyrighted software without restriction. It only requires users to include a copy of the license and copyright notice in their distributed or modified versions. This model promotes flexibility and encourages the utilization and modification of the language model codebases by researchers, developers, and companies. \\n\\nAdditionally, the \\\"within the original extended models\\\" phrase in the provided context suggests that certain language models may be distributed under licenses that allow for extensions or modifications while still requiring adherence to the original license terms. This approach enables the creation of derivative works while maintaining the integrity of the original license. \\n\\nIt is important to note that the specific licenses adopted by language model developers may vary, and it is crucial to refer to the individual licenses associated with each language model to understand the precise distribution and usage rights granted.\",\n          \"The primary objective of prompt engineering in language models is to enhance alignment and steerability, thus directing the model's performance toward a specific desired task. This process involves careful prompt design strategies, specifically for autoregressive language models, to ensure the model understands and generates responses that align with the task at hand.\",\n          \"The development of generative language models by Anthropic was influenced by a few key factors. Firstly, the company recognized the potential of generative models in creating content in various modalities beyond just text. This includes generating images, videos, and other forms of data. Secondly, Anthropic aimed to build language models that could perform well on both discriminative and generative tasks. This involves a focus on commonsense reasoning and incorporating evidence from language models to make informed decisions or generate relevant responses. \\n\\nThe cited references in the provided context also offer insights into the influences on Anthropic's language model development. The first reference, \\\"[202] A. Wan, E. Wallace, and D. Klein, \\u201cWhat evidence do language models,\\\" suggests that Anthropic considered the work of these researchers in their own models' development. Lastly, Anthropic likely considered the capabilities and limitations of rule-based approaches and human-generated content, aiming to fill any gaps or shortcomings with their generative models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10632820673708576,\n        \"min\": 0.33605989013866305,\n        \"max\": 0.9308068541941024,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.715253524721575,\n          0.880284976914172,\n          0.8682200146021544\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08100175012781397,\n        \"min\": 0.5116811990737915,\n        \"max\": 0.9190035462379456,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.537634015083313,\n          0.7406929731369019,\n          0.6472151279449463\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04631671495100621,\n        \"min\": 0.5514875650405884,\n        \"max\": 0.76991206407547,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6468048691749573,\n          0.6412896513938904,\n          0.658871591091156\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05721640183899549,\n        \"min\": 0.5540348291397095,\n        \"max\": 0.8378771543502808,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.587188184261322,\n          0.6874163746833801,\n          0.6529912948608398\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df1_eng_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d8ced050-2127-4df6-a8a8-371b6db263a4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_research</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models (LLMs) have become a cor...</td>\n",
       "      <td>0.727397</td>\n",
       "      <td>0.547440</td>\n",
       "      <td>0.675203</td>\n",
       "      <td>0.604646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>Large language models, also known as transform...</td>\n",
       "      <td>0.681756</td>\n",
       "      <td>0.586434</td>\n",
       "      <td>0.641642</td>\n",
       "      <td>0.612797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Large language models have evolved through var...</td>\n",
       "      <td>0.721333</td>\n",
       "      <td>0.630558</td>\n",
       "      <td>0.667987</td>\n",
       "      <td>0.648733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>Sure! Here are some examples of large language...</td>\n",
       "      <td>0.647905</td>\n",
       "      <td>0.533290</td>\n",
       "      <td>0.670013</td>\n",
       "      <td>0.593884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>The distribution and usage rights of source-av...</td>\n",
       "      <td>0.715254</td>\n",
       "      <td>0.652477</td>\n",
       "      <td>0.725590</td>\n",
       "      <td>0.687094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8ced050-2127-4df6-a8a8-371b6db263a4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d8ced050-2127-4df6-a8a8-371b6db263a4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d8ced050-2127-4df6-a8a8-371b6db263a4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-21a9bdbd-c09e-4a6b-a270-fbe3d6d23521\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21a9bdbd-c09e-4a6b-a270-fbe3d6d23521')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-21a9bdbd-c09e-4a6b-a270-fbe3d6d23521 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                 rag_answer_research  cos_similarity  \\\n",
       "0  Large language models (LLMs) have become a cor...        0.727397   \n",
       "1  Large language models, also known as transform...        0.681756   \n",
       "2  Large language models have evolved through var...        0.721333   \n",
       "3  Sure! Here are some examples of large language...        0.647905   \n",
       "7  The distribution and usage rights of source-av...        0.715254   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.547440     0.675203  0.604646  \n",
       "1        0.586434     0.641642  0.612797  \n",
       "2        0.630558     0.667987  0.648733  \n",
       "3        0.533290     0.670013  0.593884  \n",
       "7        0.652477     0.725590  0.687094  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df1_eng_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "qg6kQv9iFlZe",
    "outputId": "aa95b47c-58bb-4b47-ac40-980daffb9ea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df1_eng_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"eng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7432635002374021,\n        \"max\": 0.7432635002374021,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7432635002374021\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6267867260254346,\n        \"max\": 0.6267867260254346,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6267867260254346\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6630993829323695,\n        \"max\": 0.6630993829323695,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6630993829323695\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6426716699050024,\n        \"max\": 0.6426716699050024,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6426716699050024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6929675850712023,\n        \"max\": 0.6929675850712023,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6929675850712023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df1_eng_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-68b5df3e-941c-488d-9a9f-9f802b3ee023\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>eng</td>\n",
       "      <td>0.743264</td>\n",
       "      <td>0.626787</td>\n",
       "      <td>0.663099</td>\n",
       "      <td>0.642672</td>\n",
       "      <td>0.692968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68b5df3e-941c-488d-9a9f-9f802b3ee023')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-68b5df3e-941c-488d-9a9f-9f802b3ee023 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-68b5df3e-941c-488d-9a9f-9f802b3ee023');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_da037ca4-d25a-437e-a79d-3701651f7901\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df1_eng_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_da037ca4-d25a-437e-a79d-3701651f7901 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df1_eng_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         1      eng            0.743264            0.626787         0.663099   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.642672        0.692968  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df1_eng_overall_metrics = create_overall_metrics_df(val_df1_eng_eval, 1, 'eng')\n",
    "val_df1_eng_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2staAVb-pUxG"
   },
   "source": [
    "###### Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvmKpyi-pgAl"
   },
   "outputs": [],
   "source": [
    "val_df1_mkt = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYsOCIO0pk0-"
   },
   "outputs": [],
   "source": [
    "rag1_answers_marketing = []\n",
    "for question in val_df1_mkt.iloc[:10,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNMTNCeKp872"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96BSDZ_va5Xx"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkkebcrbbBGw"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfV-I4GQdPgm"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKqnvRxnA0jo"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Wx40eJXA0yh"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7Y_fu_ZA1CJ"
   },
   "outputs": [],
   "source": [
    "for question in val_df1_mkt.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_1_mkt.invoke(question)\n",
    "  rag1_answers_marketing.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D8ey-FEqgnA"
   },
   "outputs": [],
   "source": [
    "val_df1_mkt['rag_answer_marketing'] = rag1_answers_marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyLjA6zdmnYu"
   },
   "outputs": [],
   "source": [
    "val_df1_mkt.to_csv('/content/drive/My Drive/290/rag_answers_mkt/rag1_answers_mkt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD5YmkgwmyH2"
   },
   "source": [
    "###### Evaluating Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDQYJiKgm0RE"
   },
   "outputs": [],
   "source": [
    "val_df1_mkt = pd.read_csv('/content/drive/My Drive/290/rag_answers_mkt/rag1_answers_mkt.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCFe2EgRm-NM"
   },
   "outputs": [],
   "source": [
    "val_df1_mkt_eval = create_eval_df(val_df1_mkt, 'mkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "g5PBNd1BnEem",
    "outputId": "28e67057-6148-4d19-bdf8-3eaef6c65b34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df1_mkt_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"The distribution and availability of language models and their underlying code are often governed by specific licensing models. These licenses determine how accessible and usable the models are to the public and businesses. \\n\\nIn the context of the sources provided, it seems that the specific licensing models for the language models in question are not explicitly stated. However, the papers mention \\\"source-available\\\" language models, indicating that the code and inner workings of these models are accessible to the public to some extent. \\n\\nThis suggests that the licensing models adopted could be open-source or semi-open-source in nature, allowing for a certain level of transparency and accessibility. Open-source licenses, for example, permit users to view, modify, and distribute the code as long as the same license is maintained. This encourages collaboration and contributes to the advancement of the field. \\n\\nIt is important to note that the specific details of the licensing models can vary and may include restrictions or requirements, such as attribution or contribution back to the original project. \\n\\nThe marketing team can emphasize this transparency and accessibility in their messaging to promote trust and innovation in their new GenAI product line.\",\n          \"The main goal of prompt engineering is to guide and align language models to perform specific tasks, like generating text or creating images, by providing clear instructions. It's like giving the model a nudge in the right direction to ensure it produces the desired output.\",\n          \"The development of generative language models by Anthropic was influenced by a few key factors. Firstly, the goal was to create models that could generate human-like content while also exhibiting a strong understanding of commonsense reasoning. This involved training models to make decisions based on human preferences and understanding context beyond just rule-based approaches. \\n\\nAnother factor was the desire to have models perform well across both discriminative and generative tasks, showcasing their versatility and adaptability. This is a unique challenge, as most models are often trained for specific tasks and can struggle with broader applications. \\n\\nFinally, the influence of previous research in this area, such as the work by Wan, Wallace, and Klein, who explored the evidence used by language models to make decisions, also played a part in shaping Anthropic's approach to developing these generative models. \\n\\nSo, in summary, the key influences were creating human-like content with commonsense reasoning, achieving strong performance across task types, and building upon existing research in the field.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12884194420523096,\n        \"min\": 0.3407438982202718,\n        \"max\": 0.9210974827318115,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.4235004873166783,\n          0.8216941839136601,\n          0.8096523057785907\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0706165051338591,\n        \"min\": 0.34296974539756775,\n        \"max\": 0.708763837814331,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5029009580612183,\n          0.654753565788269,\n          0.6555553078651428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06320378604501505,\n        \"min\": 0.5501623749732971,\n        \"max\": 0.796379566192627,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6747329831123352,\n          0.7807887196540833,\n          0.7501866221427917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0616427873012088,\n        \"min\": 0.42631301283836365,\n        \"max\": 0.7137513756752014,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5762807130813599,\n          0.7122384309768677,\n          0.69968581199646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df1_mkt_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-82507acf-d239-4b4f-aa04-0827f1a36b76\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_marketing</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models are an essential compone...</td>\n",
       "      <td>0.808528</td>\n",
       "      <td>0.514836</td>\n",
       "      <td>0.682134</td>\n",
       "      <td>0.586793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>Large language models, the foundation of GenAI...</td>\n",
       "      <td>0.628176</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>0.604558</td>\n",
       "      <td>0.549246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Large language models, which are at the core o...</td>\n",
       "      <td>0.649927</td>\n",
       "      <td>0.534834</td>\n",
       "      <td>0.579263</td>\n",
       "      <td>0.556163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>Some well-known large language models and thei...</td>\n",
       "      <td>0.468835</td>\n",
       "      <td>0.342970</td>\n",
       "      <td>0.563165</td>\n",
       "      <td>0.426313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>The distribution and availability of language ...</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.524156</td>\n",
       "      <td>0.629998</td>\n",
       "      <td>0.572224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82507acf-d239-4b4f-aa04-0827f1a36b76')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-82507acf-d239-4b4f-aa04-0827f1a36b76 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-82507acf-d239-4b4f-aa04-0827f1a36b76');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-52ba8ebd-a971-444a-ba92-6cb139874e6a\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52ba8ebd-a971-444a-ba92-6cb139874e6a')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-52ba8ebd-a971-444a-ba92-6cb139874e6a button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                rag_answer_marketing  cos_similarity  \\\n",
       "0  Large language models are an essential compone...        0.808528   \n",
       "1  Large language models, the foundation of GenAI...        0.628176   \n",
       "2  Large language models, which are at the core o...        0.649927   \n",
       "3  Some well-known large language models and thei...        0.468835   \n",
       "7  The distribution and availability of language ...        0.423500   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.514836     0.682134  0.586793  \n",
       "1        0.503207     0.604558  0.549246  \n",
       "2        0.534834     0.579263  0.556163  \n",
       "3        0.342970     0.563165  0.426313  \n",
       "7        0.524156     0.629998  0.572224  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df1_mkt_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "zHG3dGR1nI-Q",
    "outputId": "287b0071-9ab8-4851-ba40-6f45e88d20ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df1_mkt_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"mkt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7077689472761322,\n        \"max\": 0.7077689472761322,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7077689472761322\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5584719576514684,\n        \"max\": 0.5584719576514684,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5584719576514684\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6725606551537147,\n        \"max\": 0.6725606551537147,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6725606551537147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6084506632043765,\n        \"max\": 0.6084506632043765,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6084506632043765\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6581098052402543,\n        \"max\": 0.6581098052402543,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6581098052402543\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df1_mkt_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-4ab7fdf0-1181-4cd8-bc1c-1ccc5914ac1f\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mkt</td>\n",
       "      <td>0.707769</td>\n",
       "      <td>0.558472</td>\n",
       "      <td>0.672561</td>\n",
       "      <td>0.608451</td>\n",
       "      <td>0.65811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ab7fdf0-1181-4cd8-bc1c-1ccc5914ac1f')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4ab7fdf0-1181-4cd8-bc1c-1ccc5914ac1f button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4ab7fdf0-1181-4cd8-bc1c-1ccc5914ac1f');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_00360152-a660-4add-8fa9-3ca274f5dd4d\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df1_mkt_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_00360152-a660-4add-8fa9-3ca274f5dd4d button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df1_mkt_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         1      mkt            0.707769            0.558472         0.672561   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.608451         0.65811  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df1_mkt_overall_metrics = create_overall_metrics_df(val_df1_mkt_eval, 1, 'mkt')\n",
    "val_df1_mkt_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3fVaHMnCotj"
   },
   "source": [
    "##### Pipeline 2\n",
    "* model='Mistral'\n",
    "* embedding_model='all-distilroberta-v1'\n",
    "* Prompt A\n",
    "* temperature=0.2, max_new_tokens=500\n",
    "* chunk_size=128, overlap=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2Q6IpYpCs2X"
   },
   "outputs": [],
   "source": [
    "# build rag given configuration\n",
    "rag_pipe_2_eng = build_rag_chain(model='mistral',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='A',\n",
    "                audience='eng',\n",
    "                temperature=0.2,\n",
    "                chunk_size=128,\n",
    "                overlap=0)\n",
    "\n",
    "rag_pipe_2_mkt = build_rag_chain(model='mistral',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='A',\n",
    "                audience='mkt',\n",
    "                temperature=0.2,\n",
    "                chunk_size=128,\n",
    "                overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf1IgQhbsBf0"
   },
   "source": [
    "###### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu9AEd_HsBgE"
   },
   "outputs": [],
   "source": [
    "val_df2_eng = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmi7u2kusBgE"
   },
   "outputs": [],
   "source": [
    "rag2_answers_research = []\n",
    "for question in val_df2_eng.iloc[:10,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQpAmMnjsBgE"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhJl-HW4sBgF"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuvAcTVRsBgF"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJZiLkXgsBgF"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy6WyVZtBF0o"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3k83x2MBGAU"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMpa9CZ0BGMO"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_eng.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_2_eng.invoke(question)\n",
    "  rag2_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2Qzn7tssBgF"
   },
   "outputs": [],
   "source": [
    "val_df2_eng['rag_answer_research'] = rag2_answers_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkPbbs_wsBgF"
   },
   "outputs": [],
   "source": [
    "val_df2_eng.to_csv('/content/drive/My Drive/290/rag_answers_eng/rag2_answers_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s45Z6CXSsBgG"
   },
   "source": [
    "###### Evaluating Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO2ZgoPisBgG"
   },
   "outputs": [],
   "source": [
    "val_df2_eng = pd.read_csv('/content/drive/My Drive/290/rag_answers_eng/rag2_answers_eng.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RSmUH6jNsBgG"
   },
   "outputs": [],
   "source": [
    "val_df2_eng_eval = create_eval_df(val_df2_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "gOXH1GSesBgG",
    "outputId": "c4cbb8c9-d5f7-49d1-fec7-be5393a4660c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df2_eng_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"In the context provided, no specific details about licensing models for distributing source-available language models are mentioned. However, given the focus on deploying language models in specific domains with potential regulatory implications, it can be assumed that restrictive licenses might be used to limit access to these models within certain organizations. This could help manage risks associated with regulation while ensuring the language models remain helpful by being available to those who need them most.\",\n          \"In the context provided, prompt engineering for language models refers to strategically designing prompts to modify their behavior towards specific tasks, as outlined in \\\"Of Large Language Models: A Study on Prompt Design Strategies\\\" (EMNLP Findings, 2023) and \\\"Large Language Models Are Human-Level Prompt Engineers\\\" by Zhou et al. (ICLR 2023). The primary goal is to effectively guide these models to perform desired functions while maintaining human-like interaction and understanding.\",\n          \"The context suggests that Zhaojian Lin, Andrea Madotto, and Pascale Fung explored various generative language models, specifically focusing on Recurrent Array Generators (RAG) models within extended architectures due to their effectiveness in natural language generation tasks. The choice was likely driven by RAG models' ability to handle long-term dependencies well and generate coherent text sequences. However, the context does not explicitly state that these researchers work for Anthropic, just that they published a paper on this topic.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10459943164312632,\n        \"min\": 0.45956077616817964,\n        \"max\": 0.9203545373890815,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.900623248083992,\n          0.8602551702758943,\n          0.69811245658656\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05328500507330768,\n        \"min\": 0.4822666049003601,\n        \"max\": 0.7228877544403076,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6614240407943726,\n          0.6815700531005859,\n          0.658978283405304\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04227147946832212,\n        \"min\": 0.555781900882721,\n        \"max\": 0.7349363565444946,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6243889331817627,\n          0.5869556665420532,\n          0.6451717019081116\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0421317674272486,\n        \"min\": 0.5164210796356201,\n        \"max\": 0.7246576547622681,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6726824045181274,\n          0.6307344436645508,\n          0.6520019173622131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df2_eng_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-22036a0c-2e7c-4fec-871a-040589bfba8c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_research</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>In Natural Language Processing (NLP), Large La...</td>\n",
       "      <td>0.778479</td>\n",
       "      <td>0.674721</td>\n",
       "      <td>0.666606</td>\n",
       "      <td>0.670639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>In the context provided, large language models...</td>\n",
       "      <td>0.756729</td>\n",
       "      <td>0.621127</td>\n",
       "      <td>0.607627</td>\n",
       "      <td>0.614303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>The context suggests that large language model...</td>\n",
       "      <td>0.656299</td>\n",
       "      <td>0.574878</td>\n",
       "      <td>0.565427</td>\n",
       "      <td>0.570113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>In the late 1990s, IBM introduced Deep Blue, n...</td>\n",
       "      <td>0.658438</td>\n",
       "      <td>0.559185</td>\n",
       "      <td>0.681116</td>\n",
       "      <td>0.614157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>In the context provided, no specific details a...</td>\n",
       "      <td>0.900623</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.658643</td>\n",
       "      <td>0.669568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22036a0c-2e7c-4fec-871a-040589bfba8c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-22036a0c-2e7c-4fec-871a-040589bfba8c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-22036a0c-2e7c-4fec-871a-040589bfba8c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ba9ab544-4de1-41b5-aece-dd388238c201\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba9ab544-4de1-41b5-aece-dd388238c201')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ba9ab544-4de1-41b5-aece-dd388238c201 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                 rag_answer_research  cos_similarity  \\\n",
       "0  In Natural Language Processing (NLP), Large La...        0.778479   \n",
       "1  In the context provided, large language models...        0.756729   \n",
       "2  The context suggests that large language model...        0.656299   \n",
       "3  In the late 1990s, IBM introduced Deep Blue, n...        0.658438   \n",
       "7  In the context provided, no specific details a...        0.900623   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.674721     0.666606  0.670639  \n",
       "1        0.621127     0.607627  0.614303  \n",
       "2        0.574878     0.565427  0.570113  \n",
       "3        0.559185     0.681116  0.614157  \n",
       "7        0.680862     0.658643  0.669568  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df2_eng_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "5inW3bRPsBgH",
    "outputId": "92f35ff4-6c9c-45fc-88e2-ff55a175e42b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df2_eng_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"eng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7505308625985457,\n        \"max\": 0.7505308625985457,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7505308625985457\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6316233317439373,\n        \"max\": 0.6316233317439373,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6316233317439373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6336037894854178,\n        \"max\": 0.6336037894854178,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6336037894854178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6316628135167636,\n        \"max\": 0.6316628135167636,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6316628135167636\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6910968380576546,\n        \"max\": 0.6910968380576546,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6910968380576546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df2_eng_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d7892251-e35d-4eaf-9d05-044c5bc00795\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>eng</td>\n",
       "      <td>0.750531</td>\n",
       "      <td>0.631623</td>\n",
       "      <td>0.633604</td>\n",
       "      <td>0.631663</td>\n",
       "      <td>0.691097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7892251-e35d-4eaf-9d05-044c5bc00795')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d7892251-e35d-4eaf-9d05-044c5bc00795 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d7892251-e35d-4eaf-9d05-044c5bc00795');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_e5df25e6-b3b7-48a9-a409-ad4bfc899364\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df2_eng_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_e5df25e6-b3b7-48a9-a409-ad4bfc899364 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df2_eng_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         2      eng            0.750531            0.631623         0.633604   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.631663        0.691097  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df2_eng_overall_metrics = create_overall_metrics_df(val_df2_eng_eval, 2, 'eng')\n",
    "val_df2_eng_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BqmILepsBgH"
   },
   "source": [
    "###### Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy61nKThsBgH"
   },
   "outputs": [],
   "source": [
    "val_df2_mkt = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLeFomZ8sBgH"
   },
   "outputs": [],
   "source": [
    "rag2_answers_marketing = []\n",
    "for question in val_df2_mkt.iloc[:10,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ6LXGWSsBgI"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od9gFVgYsBgI"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7svo0-G6sBgI"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CPKnwcCsBgI"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqSbUVHmBrpy"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH26vcpmBr0C"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8WQz1nmBr-q"
   },
   "outputs": [],
   "source": [
    "for question in val_df2_mkt.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_2_mkt.invoke(question)\n",
    "  rag2_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg_PRrE-sBgJ"
   },
   "outputs": [],
   "source": [
    "val_df2_mkt['rag_answer_marketing'] = rag2_answers_marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNGt5meRsBgJ"
   },
   "outputs": [],
   "source": [
    "val_df2_mkt.to_csv('/content/drive/My Drive/290/rag_answers_mkt/rag2_answers_mkt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLvdrZ2EsBgJ"
   },
   "source": [
    "###### Evaluating Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMshW24ssBgJ"
   },
   "outputs": [],
   "source": [
    "val_df2_mkt = pd.read_csv('/content/drive/My Drive/290/rag_answers_mkt/rag2_answers_mkt.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5vGhp8iFsBgJ"
   },
   "outputs": [],
   "source": [
    "val_df2_mkt_eval = create_eval_df(val_df2_mkt, 'mkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "v6zJeErJsBgJ",
    "outputId": "009d9b91-2682-4c9a-8cc9-cb0efd4eab8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df2_mkt_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"The distribution of source-available language models often follows various licensing models. Some models may be open-source, allowing free usage, modification, and redistribution. Others might require payment for commercial applications while permitting non-commercial uses. These licenses aim to balance innovation, accessibility, and revenue generation within the GenAI community.\",\n          \"In simple terms, prompt engineering for language models refers to designing effective instructions, or prompts, to influence the output of these models. According to recent research, including studies from EMNLP Findings 2023 and ICLR 2023 by Zhou et al., this approach allows us to modify the behavior of language models, making them more versatile and adaptable to various tasks.\",\n          \"Based on the context provided, there are no specific mentions of \\\"Anthropic\\\" or factors influencing its development of generative language models. Instead, the text discusses research by Zhaojian Lin, Andrea Madotto, and Pascale Fung on various natural language generation models, specifically focusing on Recursive Autoencoder Gated (RAG) models due to their versatility within language generation tasks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14038342794887312,\n        \"min\": 0.3145504614913683,\n        \"max\": 0.9584666153617137,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.3145504614913683,\n          0.838836277794713,\n          0.7457040769248391\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07984865144428484,\n        \"min\": 0.3350270688533783,\n        \"max\": 0.7479034662246704,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5636215209960938,\n          0.6449437737464905,\n          0.6715112328529358\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07421536889772103,\n        \"min\": 0.36590954661369324,\n        \"max\": 0.823250412940979,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6752471327781677,\n          0.6572553515434265,\n          0.7989976406097412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07168841969778404,\n        \"min\": 0.34978798031806946,\n        \"max\": 0.7654050588607788,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6144053936004639,\n          0.6510413289070129,\n          0.7297282814979553\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df2_mkt_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-63a3d890-5fbd-45c5-ae99-1f65e1405dd3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_marketing</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models are powerful tools in Na...</td>\n",
       "      <td>0.841659</td>\n",
       "      <td>0.578948</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.626883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns by analyzing vas...</td>\n",
       "      <td>0.770946</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.654722</td>\n",
       "      <td>0.638780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Based on the context provided, large language ...</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.528852</td>\n",
       "      <td>0.532895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>Absolutely. Before 2017, notable large languag...</td>\n",
       "      <td>0.314909</td>\n",
       "      <td>0.335027</td>\n",
       "      <td>0.365910</td>\n",
       "      <td>0.349788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>The distribution of source-available language ...</td>\n",
       "      <td>0.314550</td>\n",
       "      <td>0.642178</td>\n",
       "      <td>0.674476</td>\n",
       "      <td>0.657931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63a3d890-5fbd-45c5-ae99-1f65e1405dd3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-63a3d890-5fbd-45c5-ae99-1f65e1405dd3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-63a3d890-5fbd-45c5-ae99-1f65e1405dd3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-736cdfdb-829d-47bb-ba79-eeffe44b90c3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-736cdfdb-829d-47bb-ba79-eeffe44b90c3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-736cdfdb-829d-47bb-ba79-eeffe44b90c3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                rag_answer_marketing  cos_similarity  \\\n",
       "0  Large language models are powerful tools in Na...        0.841659   \n",
       "1  A large language model learns by analyzing vas...        0.770946   \n",
       "2  Based on the context provided, large language ...        0.668196   \n",
       "3  Absolutely. Before 2017, notable large languag...        0.314909   \n",
       "7  The distribution of source-available language ...        0.314550   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.578948     0.683473  0.626883  \n",
       "1        0.623596     0.654722  0.638780  \n",
       "2        0.537000     0.528852  0.532895  \n",
       "3        0.335027     0.365910  0.349788  \n",
       "7        0.642178     0.674476  0.657931  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df2_mkt_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "R87N1DSTsBgK",
    "outputId": "60009f21-7f60-413d-b202-aaeed105e1d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df2_mkt_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"mkt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7277115281451111,\n        \"max\": 0.7277115281451111,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7277115281451111\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5978137357876852,\n        \"max\": 0.5978137357876852,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5978137357876852\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6537813676091341,\n        \"max\": 0.6537813676091341,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6537813676091341\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6231128074801885,\n        \"max\": 0.6231128074801885,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6231128074801885\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6754121678126498,\n        \"max\": 0.6754121678126498,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6754121678126498\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df2_mkt_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c68b7281-0e48-4911-9fdd-f2fef7f85a23\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>mkt</td>\n",
       "      <td>0.727712</td>\n",
       "      <td>0.597814</td>\n",
       "      <td>0.653781</td>\n",
       "      <td>0.623113</td>\n",
       "      <td>0.675412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c68b7281-0e48-4911-9fdd-f2fef7f85a23')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c68b7281-0e48-4911-9fdd-f2fef7f85a23 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c68b7281-0e48-4911-9fdd-f2fef7f85a23');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_e3921553-b3c9-4191-8019-cde1172b610b\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df2_mkt_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_e3921553-b3c9-4191-8019-cde1172b610b button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df2_mkt_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         2      mkt            0.727712            0.597814         0.653781   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.623113        0.675412  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df2_mkt_overall_metrics = create_overall_metrics_df(val_df2_mkt_eval, 2, 'mkt')\n",
    "val_df2_mkt_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H1mlvR_GGgX"
   },
   "source": [
    "##### Pipeline 3\n",
    "* model='Mistral'\n",
    "* embedding_model='multi-qa-mpnet-base-dot-v1'\n",
    "* Prompt B\n",
    "* temperature=0.6\n",
    "* chunk_size=300, overlap=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byWzW_FQYsL5"
   },
   "outputs": [],
   "source": [
    "# build rag given configuration\n",
    "rag_pipe_3_eng = build_rag_chain(model='mistral',\n",
    "                embedding_model='multi-qa-mpnet-base-dot-v1',\n",
    "                prompt='B',\n",
    "                audience='eng',\n",
    "                temperature=0.6,\n",
    "                chunk_size=300,\n",
    "                overlap=40)\n",
    "\n",
    "rag_pipe_3_mkt = build_rag_chain(model='mistral',\n",
    "                embedding_model='multi-qa-mpnet-base-dot-v1',\n",
    "                prompt='B',\n",
    "                audience='mkt',\n",
    "                temperature=0.6,\n",
    "                chunk_size=300,\n",
    "                overlap=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqbpMnsvGGgR"
   },
   "source": [
    "###### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvyWq-IWGGgR"
   },
   "outputs": [],
   "source": [
    "val_df3_eng = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2XyYA4vGGgS"
   },
   "outputs": [],
   "source": [
    "rag3_answers_research = []\n",
    "for question in val_df3_eng.iloc[:10,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecK82annGGgS"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evn1nJmQGGgS"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXYBhUibGGgT"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH9ABytSGGgT"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgqxTojSB2Z7"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haxl5sCBB2jZ"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUdBfHibB2tz"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_eng.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_3_eng.invoke(question)\n",
    "  rag3_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6dcyW3sGGgT"
   },
   "outputs": [],
   "source": [
    "val_df3_eng['rag_answer_research'] = rag3_answers_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yf9UYqROGGgT"
   },
   "outputs": [],
   "source": [
    "val_df3_eng.to_csv('/content/drive/My Drive/290/rag_answers_eng/rag3_answers_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onM7uGEOGGgT"
   },
   "source": [
    "###### Evaluating Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaTrcAquGGgU"
   },
   "outputs": [],
   "source": [
    "val_df3_eng = pd.read_csv('/content/drive/My Drive/290/rag_answers_eng/rag3_answers_eng.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5GrJJLY7GGgU"
   },
   "outputs": [],
   "source": [
    "val_df3_eng_eval = create_eval_df(val_df3_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "znrOucbpUSn5",
    "outputId": "e7be8fbe-2202-4a80-a123-0a475eb22f71"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df3_eng_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Given the context, there seems to be no clear consensus on licensing models for distributing source-available language models like those based on Transformer architecture, such as BERT or RoBERTa. Some models, including Google's BERT, are released under an Apache License, allowing free usage but requiring attribution. Others remain proprietary, restricting their application primarily to the organization developing them. Open sourcing these models poses challenges due to potential misuse in sensitive areas like politics or law enforcement. Conversely, limiting access confines innovation and progress within select organizations.\",\n          \"The primary objective of prompt engineering in language models, as outlined in the context, is to ensure model alignment and steerability without altering model weights. This involves communicating effectively with the model through well-crafted prompts to elicit the desired outcomes while maintaining truthfulness and assistance to users. The effectiveness of these techniques can significantly differ between various models, necessitating probing their responsiveness to diverse types of prompts and tasks including those presented in different languages or related to coding.\",\n          \"The development of generative language models by Anthropic was significantly influenced by advancements in the field of Natural Language Processing (NLP), specifically the transition from purely statistical models like word n-gram language models towards more sophisticated approaches. Research conducted by Petroni et al. (2020) highlighted the importance of incorporating relevant context into these models to improve their performance. This study led to innovations in areas like pre-training techniques and multitask learning, which were key components of Anthropic's approach. Additionally, funding from organizations like the Knight-Hennessy Graduate Fellowship, CIFAR, and the Stanford Accelerator for Learning facilitated this research progress.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07971829919609714,\n        \"min\": 0.505666446211944,\n        \"max\": 0.9219165285843852,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.7455829464877147,\n          0.8918052680830012,\n          0.7159176882090944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.061708255862609224,\n        \"min\": 0.4506630003452301,\n        \"max\": 0.7127490639686584,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5332766771316528,\n          0.6114968657493591,\n          0.6819595098495483\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050617995152481256,\n        \"min\": 0.5086211562156677,\n        \"max\": 0.7872523069381714,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6445009708404541,\n          0.6128330230712891,\n          0.6748876571655273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05088755378097967,\n        \"min\": 0.4928770661354065,\n        \"max\": 0.7304867506027222,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5836370587348938,\n          0.6121641397476196,\n          0.6784051656723022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df3_eng_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f7b7c366-60eb-46ba-ab5d-26cddcfc96b3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_research</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>In the realm of Natural Language Processing (N...</td>\n",
       "      <td>0.883840</td>\n",
       "      <td>0.612936</td>\n",
       "      <td>0.675153</td>\n",
       "      <td>0.642542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>In the context provided, we don't find an exac...</td>\n",
       "      <td>0.770484</td>\n",
       "      <td>0.568004</td>\n",
       "      <td>0.564551</td>\n",
       "      <td>0.566273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>The context suggests that large language model...</td>\n",
       "      <td>0.762287</td>\n",
       "      <td>0.580499</td>\n",
       "      <td>0.590225</td>\n",
       "      <td>0.585322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>In the context provided, we find mentions of s...</td>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.450663</td>\n",
       "      <td>0.543817</td>\n",
       "      <td>0.492877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>Given the context, there seems to be no clear ...</td>\n",
       "      <td>0.745583</td>\n",
       "      <td>0.685182</td>\n",
       "      <td>0.633318</td>\n",
       "      <td>0.658230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7b7c366-60eb-46ba-ab5d-26cddcfc96b3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f7b7c366-60eb-46ba-ab5d-26cddcfc96b3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f7b7c366-60eb-46ba-ab5d-26cddcfc96b3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9e323aba-cc19-42b8-8037-937c76096e49\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e323aba-cc19-42b8-8037-937c76096e49')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9e323aba-cc19-42b8-8037-937c76096e49 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                 rag_answer_research  cos_similarity  \\\n",
       "0  In the realm of Natural Language Processing (N...        0.883840   \n",
       "1  In the context provided, we don't find an exac...        0.770484   \n",
       "2  The context suggests that large language model...        0.762287   \n",
       "3  In the context provided, we find mentions of s...        0.690561   \n",
       "7  Given the context, there seems to be no clear ...        0.745583   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.612936     0.675153  0.642542  \n",
       "1        0.568004     0.564551  0.566273  \n",
       "2        0.580499     0.590225  0.585322  \n",
       "3        0.450663     0.543817  0.492877  \n",
       "7        0.685182     0.633318  0.658230  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df3_eng_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "EHQwlOpyGGgU",
    "outputId": "01887f93-0bb0-41dd-fbe1-351ffd2bc478"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df3_eng_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"eng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7816742740743748,\n        \"max\": 0.7816742740743748,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7816742740743748\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6104232858006771,\n        \"max\": 0.6104232858006771,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6104232858006771\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6443585260556295,\n        \"max\": 0.6443585260556295,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6443585260556295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6257652525718396,\n        \"max\": 0.6257652525718396,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6257652525718396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7037197633231072,\n        \"max\": 0.7037197633231072,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7037197633231072\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df3_eng_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f3fb53a2-55fc-4576-b163-42513d2381d4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>eng</td>\n",
       "      <td>0.781674</td>\n",
       "      <td>0.610423</td>\n",
       "      <td>0.644359</td>\n",
       "      <td>0.625765</td>\n",
       "      <td>0.70372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3fb53a2-55fc-4576-b163-42513d2381d4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f3fb53a2-55fc-4576-b163-42513d2381d4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f3fb53a2-55fc-4576-b163-42513d2381d4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_73b16e42-4a5c-4e27-9531-ca261cb39b03\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df3_eng_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_73b16e42-4a5c-4e27-9531-ca261cb39b03 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df3_eng_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         3      eng            0.781674            0.610423         0.644359   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.625765         0.70372  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df3_eng_overall_metrics = create_overall_metrics_df(val_df3_eng_eval, 3, 'eng')\n",
    "val_df3_eng_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBlPqaPBGGgU"
   },
   "source": [
    "###### Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NniTZ6oGGgV"
   },
   "outputs": [],
   "source": [
    "val_df3_mkt = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4Ynq2IlGGgV"
   },
   "outputs": [],
   "source": [
    "rag3_answers_marketing = []\n",
    "for question in val_df3_mkt.iloc[:10,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWQCdb2pGGgV"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLI_FNdDGGgV"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-YXztO7GGgW"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_c6G93JRGGgW"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8-ik3_vCBw2"
   },
   "outputs": [],
   "source": [
    "rag3_answers_marketing=val_df3_mkt.rag_answer_marketing.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZbGbheFCB4h"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KlWXRFdCCDc"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfnCCQZkCCUt"
   },
   "outputs": [],
   "source": [
    "for question in val_df3_mkt.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_3_mkt.invoke(question)\n",
    "  rag3_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dJnxWF-GGgW"
   },
   "outputs": [],
   "source": [
    "val_df3_mkt['rag_answer_marketing'] = rag3_answers_marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHKX-xWNGGgW"
   },
   "outputs": [],
   "source": [
    "val_df3_mkt.to_csv('/content/drive/My Drive/290/rag_answers_mkt/rag3_answers_mkt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSaQ4leEGGgW"
   },
   "source": [
    "###### Evaluating Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G9S8ZCFGGgX"
   },
   "outputs": [],
   "source": [
    "val_df3_mkt = pd.read_csv('/content/drive/My Drive/290/rag_answers_mkt/rag3_answers_mkt.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "L0aDgNENGGgX"
   },
   "outputs": [],
   "source": [
    "val_df3_mkt_eval = create_eval_df(val_df3_mkt, 'mkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "S_lLMOVLGGgX",
    "outputId": "9ca6bc75-da29-453c-ecfd-ebafca94ba72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df3_mkt_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"In the realm of General Artificial Intelligence (GenAI), particularly with regard to source-available language models, there exists a significant debate around licensing. Some companies release these models under an open-source license, allowing anyone to access and modify the codebase. However, this comes with challenges, such as potential misuse in sensitive areas like politics or law enforcement. Others restrict access to ensure proper regulation and prevent harmful applications. The choice between open-source and closed licensing depends on various factors including ethical considerations, market demand, and regulatory requirements.\",\n          \"The primary objective of prompt engineering for language models is to guide their behavior towards achieving specific outcomes without altering their underlying weights. By crafting effective prompts, we aim to ensure these models remain helpful, truthful, and adaptable to various tasks while maintaining their core functionality.\",\n          \"Anthropic's development of generative language models was significantly influenced by advancements in the field, particularly the shift from traditional statistical models like word n-grams towards more advanced models based on deep learning techniques, as described in Petroni et al.'s study. These newer models allow for retrieval of relevant context during generation, leading to improved performance. Furthermore, researchers like Radford, Wu, Child, Luan, Amodei, and Sutskever paved the way with groundbreaking research in this area, which inspired ongoing innovation in generative language modeling.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10307581475656914,\n        \"min\": 0.43962010661737333,\n        \"max\": 0.9219489052704735,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.43962010661737333,\n          0.9219489052704735,\n          0.7374941908870869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07123948796762403,\n        \"min\": 0.3487023711204529,\n        \"max\": 0.70562744140625,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5374728441238403,\n          0.5521290302276611,\n          0.6270253658294678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06968466914702214,\n        \"min\": 0.45649418234825134,\n        \"max\": 0.8048513531684875,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6293849349021912,\n          0.7857838869094849,\n          0.7370549440383911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06379995771543416,\n        \"min\": 0.39538320899009705,\n        \"max\": 0.7342513203620911,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.5798090100288391,\n          0.6485535502433777,\n          0.6776025295257568\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df3_mkt_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-3e1ccbe9-a5d5-4b4e-85ac-eb3a26eb63ed\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_marketing</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models are powerful tools in Na...</td>\n",
       "      <td>0.859934</td>\n",
       "      <td>0.530634</td>\n",
       "      <td>0.582350</td>\n",
       "      <td>0.555291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model, such as those used in ...</td>\n",
       "      <td>0.737151</td>\n",
       "      <td>0.545694</td>\n",
       "      <td>0.653399</td>\n",
       "      <td>0.594709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Large language models, such as those mentioned...</td>\n",
       "      <td>0.690080</td>\n",
       "      <td>0.562414</td>\n",
       "      <td>0.593334</td>\n",
       "      <td>0.577460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>In recent years, large-scale language models l...</td>\n",
       "      <td>0.493731</td>\n",
       "      <td>0.348702</td>\n",
       "      <td>0.456494</td>\n",
       "      <td>0.395383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>In the realm of General Artificial Intelligenc...</td>\n",
       "      <td>0.439620</td>\n",
       "      <td>0.589573</td>\n",
       "      <td>0.604727</td>\n",
       "      <td>0.597054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1ccbe9-a5d5-4b4e-85ac-eb3a26eb63ed')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3e1ccbe9-a5d5-4b4e-85ac-eb3a26eb63ed button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3e1ccbe9-a5d5-4b4e-85ac-eb3a26eb63ed');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-088fe7db-a114-4758-a4ea-eedb4f66a674\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-088fe7db-a114-4758-a4ea-eedb4f66a674')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-088fe7db-a114-4758-a4ea-eedb4f66a674 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                rag_answer_marketing  cos_similarity  \\\n",
       "0  Large language models are powerful tools in Na...        0.859934   \n",
       "1  A large language model, such as those used in ...        0.737151   \n",
       "2  Large language models, such as those mentioned...        0.690080   \n",
       "3  In recent years, large-scale language models l...        0.493731   \n",
       "7  In the realm of General Artificial Intelligenc...        0.439620   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.530634     0.582350  0.555291  \n",
       "1        0.545694     0.653399  0.594709  \n",
       "2        0.562414     0.593334  0.577460  \n",
       "3        0.348702     0.456494  0.395383  \n",
       "7        0.589573     0.604727  0.597054  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df3_mkt_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "xozBf-obGGgX",
    "outputId": "f6c3be61-2d9b-44cb-ef22-01af23dfe417"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df3_mkt_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"mkt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7566383812496167,\n        \"max\": 0.7566383812496167,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7566383812496167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5548332190284362,\n        \"max\": 0.5548332190284362,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5548332190284362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6603719494663752,\n        \"max\": 0.6603719494663752,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6603719494663752\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6013188150066596,\n        \"max\": 0.6013188150066596,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6013188150066596\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6789785981281382,\n        \"max\": 0.6789785981281382,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6789785981281382\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df3_mkt_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-4c4ec554-ead5-4dbc-bc37-bd03140bcef0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>mkt</td>\n",
       "      <td>0.756638</td>\n",
       "      <td>0.554833</td>\n",
       "      <td>0.660372</td>\n",
       "      <td>0.601319</td>\n",
       "      <td>0.678979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c4ec554-ead5-4dbc-bc37-bd03140bcef0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4c4ec554-ead5-4dbc-bc37-bd03140bcef0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4c4ec554-ead5-4dbc-bc37-bd03140bcef0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_2ce27b88-4439-4db9-affe-915fc31abac9\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df3_mkt_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_2ce27b88-4439-4db9-affe-915fc31abac9 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df3_mkt_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         3      mkt            0.756638            0.554833         0.660372   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.601319        0.678979  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df3_mkt_overall_metrics = create_overall_metrics_df(val_df3_mkt_eval, 3, 'mkt')\n",
    "val_df3_mkt_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGHNZJAVZRlj"
   },
   "source": [
    "##### Pipeline 4\n",
    "* model='Cohere'\n",
    "* embedding_model='all-distilroberta-v1'\n",
    "* Prompt C\n",
    "* chunk_size=300, overlap=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_keu7hJVbxgA"
   },
   "outputs": [],
   "source": [
    "# build rag given configuration\n",
    "rag_pipe_4_eng = build_rag_chain(model='cohere',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='C',\n",
    "                audience='eng',\n",
    "                temperature=None,\n",
    "                chunk_size=300,\n",
    "                overlap=40)\n",
    "\n",
    "rag_pipe_4_mkt = build_rag_chain(model='cohere',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='C',\n",
    "                audience='mkt',\n",
    "                temperature=None,\n",
    "                chunk_size=300,\n",
    "                overlap=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlo-qP_SliLe"
   },
   "source": [
    "###### Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiQ1J3tyliLt"
   },
   "outputs": [],
   "source": [
    "val_df4_eng = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoaO5eQHliLu"
   },
   "outputs": [],
   "source": [
    "rag4_answers_research = []\n",
    "for question in val_df4_eng.iloc[:10,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTdKh9PdliLu"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O81JNBOliLv"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz6F5-Sfo0_o"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2nUKWHzliLw"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcldcEhpCOAE"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZwzw_U9COJN"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z090kAwCOTM"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_eng.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_4_eng.invoke(question)\n",
    "  rag4_answers_research.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPqu_I2NliLw"
   },
   "outputs": [],
   "source": [
    "val_df4_eng['rag_answer_research'] = rag4_answers_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOYjMsGhliLw"
   },
   "outputs": [],
   "source": [
    "val_df4_eng.to_csv('/content/drive/My Drive/290/rag_answers_eng/rag4_answers_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5yub2YCliLx"
   },
   "source": [
    "###### Evaluating Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8g-kACGliLx"
   },
   "outputs": [],
   "source": [
    "val_df4_eng = pd.read_csv('/content/drive/My Drive/290/rag_answers_eng/rag4_answers_eng.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmSSaZfpdT3y"
   },
   "outputs": [],
   "source": [
    "val_df4_eng_eval = create_eval_df(val_df4_eng, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "G4oRDveTdEZs",
    "outputId": "9c98ce0b-25ec-411a-af66-0e722ac8bf2f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df4_eng_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"The choice of licensing models for source-available language models depends on the desired level of control and the goal of fostering a community. The MIT license is a popular choice, offering few restrictions and maximum reuse, which is ideal for encouraging community contributions. Alternatively, the GPL family of licenses ensures that any modifications or derivative works are also open-sourced, providing a way to track and regulate the use of the model. A more restrictive option is a proprietary license, which allows the owner to maintain tight control over the model's distribution and modifications.\",\n          \"The primary objective of prompt engineering in language models is to guide the model's behavior and align its outputs with desired outcomes, all without requiring any changes to the underlying model weights. It is an empirical practice that involves the strategic composition and formatting of prompts to maximize a model's performance on specific tasks. The effectiveness of prompt engineering techniques can vary significantly across different models, underscoring the importance of treating it as an empirical science.\",\n          \"Anthropic's development of generative language models was influenced by advancements in fine-tuning language models from human preferences, as outlined in the work of Ziegler et al. Additionally, the company's partnership with Notion and Quora, aimed at enhancing natural language generation models, contributed to their language model development. The emergence of autonomous scientific research capabilities of large language models, as described by Boiko et al., also likely played a role in shaping Anthropic's approach to developing their language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10896258636167905,\n        \"min\": 0.37411373579013957,\n        \"max\": 0.933283569794968,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.7446681859010957,\n          0.9248120337377592,\n          0.8376438828722664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0546967820167644,\n        \"min\": 0.5341096520423889,\n        \"max\": 0.8073242902755737,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6484576463699341,\n          0.6966639161109924,\n          0.6916866898536682\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.049340118598774185,\n        \"min\": 0.5721338987350464,\n        \"max\": 0.7926462888717651,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6474790573120117,\n          0.6838521957397461,\n          0.6726508140563965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0475030931670971,\n        \"min\": 0.5767926573753357,\n        \"max\": 0.799917995929718,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6479679942131042,\n          0.6901986598968506,\n          0.6820359826087952\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df4_eng_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-54cb4961-3174-4dae-b0b9-1c0165417b94\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_research</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models (LLMs) have become pivot...</td>\n",
       "      <td>0.857932</td>\n",
       "      <td>0.675201</td>\n",
       "      <td>0.670343</td>\n",
       "      <td>0.672763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>Large language models learn by analyzing vast ...</td>\n",
       "      <td>0.809942</td>\n",
       "      <td>0.654429</td>\n",
       "      <td>0.619647</td>\n",
       "      <td>0.636563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Transformer-based architectures, such as the A...</td>\n",
       "      <td>0.652485</td>\n",
       "      <td>0.660419</td>\n",
       "      <td>0.635273</td>\n",
       "      <td>0.647602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>Some prominent examples of large language mode...</td>\n",
       "      <td>0.702908</td>\n",
       "      <td>0.588870</td>\n",
       "      <td>0.670439</td>\n",
       "      <td>0.627013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>The choice of licensing models for source-avai...</td>\n",
       "      <td>0.744668</td>\n",
       "      <td>0.705258</td>\n",
       "      <td>0.715837</td>\n",
       "      <td>0.710508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54cb4961-3174-4dae-b0b9-1c0165417b94')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-54cb4961-3174-4dae-b0b9-1c0165417b94 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-54cb4961-3174-4dae-b0b9-1c0165417b94');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7c1b7bb1-d018-4f28-ad73-dcc9d114b74e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c1b7bb1-d018-4f28-ad73-dcc9d114b74e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7c1b7bb1-d018-4f28-ad73-dcc9d114b74e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                 rag_answer_research  cos_similarity  \\\n",
       "0  Large language models (LLMs) have become pivot...        0.857932   \n",
       "1  Large language models learn by analyzing vast ...        0.809942   \n",
       "2  Transformer-based architectures, such as the A...        0.652485   \n",
       "3  Some prominent examples of large language mode...        0.702908   \n",
       "7  The choice of licensing models for source-avai...        0.744668   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.675201     0.670343  0.672763  \n",
       "1        0.654429     0.619647  0.636563  \n",
       "2        0.660419     0.635273  0.647602  \n",
       "3        0.588870     0.670439  0.627013  \n",
       "7        0.705258     0.715837  0.710508  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df4_eng_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "jOrOEBhjdGNH",
    "outputId": "632449bd-f10a-45a6-dfe1-dedd84070e50"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df4_eng_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"eng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7734113592024174,\n        \"max\": 0.7734113592024174,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7734113592024174\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6596590968278738,\n        \"max\": 0.6596590968278738,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6596590968278738\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6619996646275887,\n        \"max\": 0.6619996646275887,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6619996646275887\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.660099635903652,\n        \"max\": 0.660099635903652,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.660099635903652\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7167554975530347,\n        \"max\": 0.7167554975530347,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7167554975530347\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df4_eng_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-fc58c010-3b9e-4103-bbb0-18ab4d89bf15\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>eng</td>\n",
       "      <td>0.773411</td>\n",
       "      <td>0.659659</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.6601</td>\n",
       "      <td>0.716755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc58c010-3b9e-4103-bbb0-18ab4d89bf15')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fc58c010-3b9e-4103-bbb0-18ab4d89bf15 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fc58c010-3b9e-4103-bbb0-18ab4d89bf15');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_584818e7-9b8c-443d-8534-0a42c9e470a6\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df4_eng_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_584818e7-9b8c-443d-8534-0a42c9e470a6 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df4_eng_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         4      eng            0.773411            0.659659            0.662   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0       0.6601        0.716755  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df4_eng_overall_metrics = create_overall_metrics_df(val_df4_eng_eval, 4, 'eng')\n",
    "val_df4_eng_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8yV50UWliLy"
   },
   "source": [
    "###### Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8LckMOgliLy"
   },
   "outputs": [],
   "source": [
    "val_df4_mkt = val_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUEyTsZPliLy"
   },
   "outputs": [],
   "source": [
    "rag4_answers_marketing = []\n",
    "for question in val_df4_mkt.iloc[:10,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTOjjsVNliLz"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[10:20,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SaZSD3LliLz"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[20:30,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBzFZg-iliLz"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[30:40,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Oc0kPLuliL0"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[40:50,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzmYP-N7CaZm"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[50:60,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzDddqOXCajq"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[60:70,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l-9FIqfCaub"
   },
   "outputs": [],
   "source": [
    "for question in val_df4_mkt.iloc[70:75,:].question:\n",
    "  answer = rag_pipe_4_mkt.invoke(question)\n",
    "  rag4_answers_marketing.append(answer)\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SySRTBtliL0"
   },
   "outputs": [],
   "source": [
    "val_df4_mkt['rag_answer_marketing'] = rag4_answers_marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEnk-CLIliL0"
   },
   "outputs": [],
   "source": [
    "val_df4_mkt.to_csv('/content/drive/My Drive/290/rag_answers_mkt/rag4_answers_mkt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tR_fLedliL0"
   },
   "source": [
    "###### Evaluating Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GCxBsvcliL0"
   },
   "outputs": [],
   "source": [
    "val_df4_mkt = pd.read_csv('/content/drive/My Drive/290/rag_answers_mkt/rag4_answers_mkt.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Su6DjE6JliL1"
   },
   "outputs": [],
   "source": [
    "val_df4_mkt_eval = create_eval_df(val_df4_mkt, 'mkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "f0WjOE2YliL1",
    "outputId": "98fba5db-3e86-4a70-f3f2-000eec1ef7ba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df4_mkt_eval\",\n  \"rows\": 75,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"What licensing models have been adopted for the distribution of source-available language models?\",\n          \"What is the main goal of prompt engineering in language models?\",\n          \"What factors influenced the development of generative language models by Anthropic?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_research\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n          \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n          \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\",\n          \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\",\n          \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_answer_marketing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Currently, there is no standard licensing model for source-available language models. The field is still evolving, and the challenges of harmful applications and the high costs of fine-tuning are actively being addressed.\",\n          \"The primary goal of prompt engineering in language models is to guide and influence the model's behavior to achieve desired outcomes without changing the underlying model architecture or weights.\",\n          \"Anthropic's development of generative language models was influenced by advancements in fine-tuning language models from human preferences and the emergence of autonomous scientific research capabilities in large language models. These factors, combined with partnerships, helped enhance the model's performance in various tasks and address limitations in coding, math, and reasoning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13848898714110958,\n        \"min\": 0.24608288035158993,\n        \"max\": 0.9658817523531213,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.24608288035158993,\n          0.9658817523531213,\n          0.7793597396738992\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07384918269529368,\n        \"min\": 0.43002817034721375,\n        \"max\": 0.8369662761688232,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.6463205218315125,\n          0.6427090167999268,\n          0.6413465738296509\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0795296126995425,\n        \"min\": 0.5492360591888428,\n        \"max\": 0.9025723934173584,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.7288835644721985,\n          0.7625360488891602,\n          0.6669652462005615\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06716215653784617,\n        \"min\": 0.5118560194969177,\n        \"max\": 0.8644161820411682,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.68512362241745,\n          0.6975136399269104,\n          0.6539050936698914\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df4_mkt_eval"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-89401990-632a-4cd2-9169-11f90bf8fc1d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer_research</th>\n",
       "      <th>gold_answer_marketing</th>\n",
       "      <th>rag_answer_marketing</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What purpose do large language models serve in...</td>\n",
       "      <td>Large language models (LLMs) serve the purpose...</td>\n",
       "      <td>Large language models serve the purpose of imp...</td>\n",
       "      <td>Large language models (LLMs) are a pivotal too...</td>\n",
       "      <td>0.826107</td>\n",
       "      <td>0.563684</td>\n",
       "      <td>0.642879</td>\n",
       "      <td>0.600682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a large language model learn from tex...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>A large language model learns from text during...</td>\n",
       "      <td>Large language models learn by analyzing vast ...</td>\n",
       "      <td>0.683940</td>\n",
       "      <td>0.617790</td>\n",
       "      <td>0.660558</td>\n",
       "      <td>0.638459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some key architectures behind the dev...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Key architectures behind the development of la...</td>\n",
       "      <td>Large language models (LLMs) are built on tran...</td>\n",
       "      <td>0.776285</td>\n",
       "      <td>0.609411</td>\n",
       "      <td>0.605061</td>\n",
       "      <td>0.607228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you name some specific large language mode...</td>\n",
       "      <td>Some specific large language models include GP...</td>\n",
       "      <td>Chinchilla by DeepMind, GPT-3 by OpenAI.</td>\n",
       "      <td>Some well-known large language models include ...</td>\n",
       "      <td>0.504350</td>\n",
       "      <td>0.430028</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.511856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What licensing models have been adopted for th...</td>\n",
       "      <td>Based on the provided context, it seems that l...</td>\n",
       "      <td>Answer: Some organizations choose open-sourcin...</td>\n",
       "      <td>Currently, there is no standard licensing mode...</td>\n",
       "      <td>0.246083</td>\n",
       "      <td>0.694451</td>\n",
       "      <td>0.720447</td>\n",
       "      <td>0.707211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89401990-632a-4cd2-9169-11f90bf8fc1d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-89401990-632a-4cd2-9169-11f90bf8fc1d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-89401990-632a-4cd2-9169-11f90bf8fc1d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-086a94f3-5d1e-4cfa-9b5f-58af642a4952\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-086a94f3-5d1e-4cfa-9b5f-58af642a4952')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-086a94f3-5d1e-4cfa-9b5f-58af642a4952 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What purpose do large language models serve in...   \n",
       "1  How does a large language model learn from tex...   \n",
       "2  What are some key architectures behind the dev...   \n",
       "3  Can you name some specific large language mode...   \n",
       "7  What licensing models have been adopted for th...   \n",
       "\n",
       "                                gold_answer_research  \\\n",
       "0  Large language models (LLMs) serve the purpose...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3  Some specific large language models include GP...   \n",
       "7  Based on the provided context, it seems that l...   \n",
       "\n",
       "                               gold_answer_marketing  \\\n",
       "0  Large language models serve the purpose of imp...   \n",
       "1  A large language model learns from text during...   \n",
       "2  Key architectures behind the development of la...   \n",
       "3           Chinchilla by DeepMind, GPT-3 by OpenAI.   \n",
       "7  Answer: Some organizations choose open-sourcin...   \n",
       "\n",
       "                                rag_answer_marketing  cos_similarity  \\\n",
       "0  Large language models (LLMs) are a pivotal too...        0.826107   \n",
       "1  Large language models learn by analyzing vast ...        0.683940   \n",
       "2  Large language models (LLMs) are built on tran...        0.776285   \n",
       "3  Some well-known large language models include ...        0.504350   \n",
       "7  Currently, there is no standard licensing mode...        0.246083   \n",
       "\n",
       "   bert_precision  bert_recall   bert_f1  \n",
       "0        0.563684     0.642879  0.600682  \n",
       "1        0.617790     0.660558  0.638459  \n",
       "2        0.609411     0.605061  0.607228  \n",
       "3        0.430028     0.632143  0.511856  \n",
       "7        0.694451     0.720447  0.707211  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df4_mkt_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "y2Ix-sbUliL1",
    "outputId": "a2bae685-a9c3-4bc0-e59a-ad1689d01a4e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_df4_mkt_overall_metrics\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pipeline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"mkt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cos_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7704386902072954,\n        \"max\": 0.7704386902072954,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7704386902072954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6317414979522045,\n        \"max\": 0.6317414979522045,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6317414979522045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6790366711524817,\n        \"max\": 0.6790366711524817,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6790366711524817\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_bert_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6525144577026367,\n        \"max\": 0.6525144577026367,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6525144577026367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combined_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.711476573954966,\n        \"max\": 0.711476573954966,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.711476573954966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "val_df4_mkt_overall_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ce12f565-517b-4061-bf88-a34324cc0b68\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline</th>\n",
       "      <th>audience</th>\n",
       "      <th>avg_cos_similarity</th>\n",
       "      <th>avg_bert_precision</th>\n",
       "      <th>avg_bert_recall</th>\n",
       "      <th>avg_bert_f1</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>mkt</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.631741</td>\n",
       "      <td>0.679037</td>\n",
       "      <td>0.652514</td>\n",
       "      <td>0.711477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce12f565-517b-4061-bf88-a34324cc0b68')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ce12f565-517b-4061-bf88-a34324cc0b68 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ce12f565-517b-4061-bf88-a34324cc0b68');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_62b49bce-ab94-4c85-9e73-d725bd364dfd\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('val_df4_mkt_overall_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_62b49bce-ab94-4c85-9e73-d725bd364dfd button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('val_df4_mkt_overall_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   pipeline audience  avg_cos_similarity  avg_bert_precision  avg_bert_recall  \\\n",
       "0         4      mkt            0.770439            0.631741         0.679037   \n",
       "\n",
       "   avg_bert_f1  combined_score  \n",
       "0     0.652514        0.711477  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df4_mkt_overall_metrics = create_overall_metrics_df(val_df4_mkt_eval, 4, 'mkt')\n",
    "val_df4_mkt_overall_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdOhlN23AYiA"
   },
   "source": [
    "## 5. Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohwv_Gjd9JJq"
   },
   "source": [
    "### 5.1 Model Specifications\n",
    "\n",
    "Document the detailed specs of your choices. Also comment on how you valued the needs of the marketing team vs the needs of the researchers, in case you had to make a trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ99pFlPtIxW"
   },
   "source": [
    "For my final model, I chose the Cohere model, the embedding model multi-qa-mpnet-base-dot-v1, chunk size 128 and overlap 0, and prompt A. In the experiments with various model configurations, I compared RAG outputs with the gold standard answers for each audience. The aforementioned model performed the best, given its combined average cosine similarity and BertScore F1 score, for both audiences, across experiments. Therefore, the needs of the marketing team and researchers are equally being valued in this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7J41pWiyh06"
   },
   "source": [
    "\n",
    "### 5.2 Some Test Questions\n",
    "\n",
    "**QUESTIONS:**\n",
    "\n",
    "\n",
    "Please study the answers generated by your chosen setup for these specific test questions:\n",
    "\n",
    "1. \"What purpose do large language models serve in the field of natural language processing?\" (Question 0)\n",
    "\n",
    "2. \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\" (Question 50)\n",
    "\n",
    "3. \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\" (Question 83, no labeled answers)\n",
    "\n",
    "For each of the three questions above please provide:\n",
    "\n",
    "a) The RAG results (research and marketing response)  \n",
    "b) The context provided  \n",
    "c) The document sources for the context  \n",
    "d) Also discuss your metric(s) for the first two examples (for both responses) compared to the gold responses\n",
    "\n",
    "Then, for questions 1 and 2, comment on how well you feel your metrics captured the differences and similarities between your answer and the gold answer?\n",
    "\n",
    "Put your answers to these questions into the answers file as you have done on previous assignments. Please consult the answer file for further details.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5giLzNpc1lS"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def extract_context(text):\n",
    "    pattern = re.compile(r'Here is the context:(.*?)That is it for the context.', re.DOTALL)\n",
    "\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def final_rag_context_provided(question):\n",
    "  final_retriever = get_retriever('all-distilroberta-v1', 300, 40)\n",
    "  final_prompt = ChatPromptTemplate.from_template('[/INST]Here is the context:{context} That is it for the context.[INST]')\n",
    "  final_rag_chain = (\n",
    "    {\"context\": final_retriever | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    )\n",
    "  output = final_rag_chain.invoke(question)\n",
    "  context = extract_context(output.messages[0].content)\n",
    "  return context\n",
    "\n",
    "def get_document_sources(question):\n",
    "  vectorstore = get_retriever('all-distilroberta-v1', 300, 40).vectorstore\n",
    "  docs = vectorstore.similarity_search_by_vector(base_embeddings2.embed_query(question)) # will rank the splits\n",
    "  for doc in docs:\n",
    "    print('document: ', doc.page_content)\n",
    "    print('source: ', doc.metadata['source'])\n",
    "    print('doc_source:', doc.metadata['doc_source'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBE4aLhHuztI"
   },
   "outputs": [],
   "source": [
    "# function to build the final rag chain given the model name, embedding model name, prompt version letter, audience 3-letter indicator, temperature, chunk_size, and overlap\n",
    "def build_rag_chain(model='mistral', embedding_model='multi-qa-mpnet-base-dot-v1', prompt='A', audience='eng', temperature=0.2, chunk_size=128, overlap=0):\n",
    "  retriever = get_retriever(embedding_model, chunk_size, overlap)\n",
    "  rag_prompt = get_rag_prompt(prompt, audience)\n",
    "  llm_lc = get_llm(model, temperature)\n",
    "\n",
    "  if model=='cohere':\n",
    "    chain=build_rag_chain_cohere(retriever, rag_prompt, llm_lc)\n",
    "  if model=='mistral':\n",
    "    llm_lc= build_mistral_llm_lc(temperature=temperature)\n",
    "    chain=build_rag_chain_mistral(retriever, rag_prompt, llm_lc)\n",
    "  return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSoSdUriAa-C"
   },
   "outputs": [],
   "source": [
    "# this is the final set of rag chains I chose\n",
    "final_rag_pipe_eng = build_rag_chain(model='cohere',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='C',\n",
    "                audience='eng',\n",
    "                temperature=None,\n",
    "                chunk_size=300,\n",
    "                overlap=40)\n",
    "\n",
    "final_rag_pipe_mkt = build_rag_chain(model='cohere',\n",
    "                embedding_model='all-distilroberta-v1',\n",
    "                prompt='C',\n",
    "                audience='mkt',\n",
    "                temperature=None,\n",
    "                chunk_size=300,\n",
    "                overlap=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbTps3ICxtzP"
   },
   "source": [
    "#### 5.2.1 Test Question 1\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSCTdcmJdPzN"
   },
   "outputs": [],
   "source": [
    "test_q1=validation_questions_answers[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpujK23I8DvO"
   },
   "outputs": [],
   "source": [
    "gold_ans1_eng=validation_questions_answers[0]['gold_answer_research']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "XAHgUcHIdawy",
    "outputId": "7c04d852-802c-4ee2-856b-92bad0c76fb7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_ans1_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBKjMQGK8kNp"
   },
   "outputs": [],
   "source": [
    "gold_ans1_mkt=validation_questions_answers[0]['gold_answer_marketing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3PXoAoxddcNQ",
    "outputId": "1bac3a3b-3f64-495a-8822-3263d2a307a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_ans1_mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcP1XmuC7S3M"
   },
   "source": [
    "a) The RAG results (research and marketing response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SdO9BXg63Wz"
   },
   "outputs": [],
   "source": [
    "test_ans1_eng = final_rag_pipe_eng.invoke(test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "MMuY2iXi7Kuv",
    "outputId": "5fcb03ff-d615-43c6-8cf4-375d96db607a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models (LLMs) have become integral to natural language processing (NLP) due to their ability to learn and model complex statistical relationships from vast text corpora. This enables them to generate human-like language and perform advanced language-based tasks. The combination of large datasets, feedforward neural networks, and transformers has resulted in the current state-of-the-art performance of LLMs, surpassing that of recurrent neural network-based models and symbolic language models. They are now essential for developing advanced NLP applications and driving the field forward.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans1_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdHM8DoJ7Ffg"
   },
   "outputs": [],
   "source": [
    "test_ans1_mkt = final_rag_pipe_mkt.invoke(test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "UZjjSBvg7P9Q",
    "outputId": "bd0e16c4-cbc3-41a0-d2da-db986cb23103"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models (LLMs) are an integral part of natural language processing (NLP), as they enable machines to understand and generate human-like language. LLMs are trained on vast datasets, allowing them to learn the statistical relationships between words and generate meaningful responses, making them essential for building intelligent language-based applications.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans1_mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtZgkVW97YS-"
   },
   "source": [
    "b) The context provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "wUtSHiPS7iBz",
    "outputId": "b87af84b-39bc-4a16-b383-1b97006d7a4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from vast\\n\\nlanguage models dominated over symbolic language models, as they can usefully ingest large datasets.\\n\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the\\n\\n2School of Computer Science, Beijing University of Posts and Telecommunications\\n3School of Artificial Intelligence, Nankai University, 4Beijing Academy of Artificial Intelligence\\nyutaozhu94@gmail.com, dou@ruc.edu.cn\\nAbstract\\nLarge language models (LLMs) have demon-'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_context_provided(test_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w6rHly77Zs7"
   },
   "source": [
    "c) The document sources for the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnC9tq2W7lfJ",
    "outputId": "5653c451-8e65-42a4-c37b-43a8f83d042d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document:  A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from vast\n",
      "source:  https://en.wikipedia.org/wiki/Large_language_model\n",
      "doc_source: Wikipedia\n",
      "\n",
      "\n",
      "document:  language models dominated over symbolic language models, as they can usefully ingest large datasets.\n",
      "source:  https://en.wikipedia.org/wiki/Large_language_model\n",
      "doc_source: Wikipedia\n",
      "\n",
      "\n",
      "document:  Large language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the\n",
      "source:  https://en.wikipedia.org/wiki/Language_model\n",
      "doc_source: Wikipedia\n",
      "\n",
      "\n",
      "document:  2School of Computer Science, Beijing University of Posts and Telecommunications\n",
      "3School of Artificial Intelligence, Nankai University, 4Beijing Academy of Artificial Intelligence\n",
      "yutaozhu94@gmail.com, dou@ruc.edu.cn\n",
      "Abstract\n",
      "Large language models (LLMs) have demon-\n",
      "source:  https://arxiv.org/pdf/2401.06532.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_document_sources(test_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAK3ytaB7bD7"
   },
   "source": [
    "d) Also discuss your metric(s) for the first two examples (for both responses) compared to the gold responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfiGaPPI80vZ"
   },
   "outputs": [],
   "source": [
    "eng_cos_sim1=get_cosine_similarity(gold_ans1_eng,test_ans1_eng)\n",
    "eng_bert_score1=get_bert_score(gold_ans1_eng,test_ans1_eng)\n",
    "eng_combined_score1=(eng_cos_sim1+eng_bert_score1['f1'])/2\n",
    "\n",
    "mkt_cos_sim1=get_cosine_similarity(gold_ans1_mkt,test_ans1_mkt)\n",
    "mkt_bert_score1=get_bert_score(gold_ans1_mkt,test_ans1_mkt)\n",
    "mkt_combined_score1=(mkt_cos_sim1+mkt_bert_score1['f1'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arF9ixgF7ufL",
    "outputId": "7bb7a1f7-3e8c-49f0-cd04-d8d83e4460de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Evaluation:\n",
      "cosine similarity: 0.9100002597590183\n",
      "bert score: {'precision': 0.6556969881057739, 'recall': 0.6663129329681396, 'f1': 0.6609623432159424}\n",
      "combined score: 0.7854813014874804\n",
      "\n",
      "\n",
      "\n",
      "Marketing Evaluation:\n",
      "cosine similarity: 0.738350294083111\n",
      "bert score: {'precision': 0.5388528108596802, 'recall': 0.586467981338501, 'f1': 0.5616530179977417}\n",
      "combined score: 0.6500016560404264\n"
     ]
    }
   ],
   "source": [
    "print('Engineering Evaluation:')\n",
    "print('cosine similarity:', eng_cos_sim1)\n",
    "print('bert score:', eng_bert_score1)\n",
    "print('combined score:', eng_combined_score1)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Marketing Evaluation:')\n",
    "print('cosine similarity:', mkt_cos_sim1)\n",
    "print('bert score:', mkt_bert_score1)\n",
    "print('combined score:', mkt_combined_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6wrr5VUUJk1"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZdkNySoUDy3"
   },
   "source": [
    "#### 5.2.2 Test Question 2\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDvguPmQ-CXr"
   },
   "outputs": [],
   "source": [
    "test_q2=validation_questions_answers[50]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3YbL1Im-CXs"
   },
   "outputs": [],
   "source": [
    "gold_ans2_eng=validation_questions_answers[50]['gold_answer_research']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "iKj28XDye_7D",
    "outputId": "685414ae-6ccc-425e-884d-b01514008ed8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_ans2_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxiCuZyX-CXs"
   },
   "outputs": [],
   "source": [
    "gold_ans2_mkt=validation_questions_answers[50]['gold_answer_marketing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "qUZE1qp_fAdd",
    "outputId": "592ddceb-1b7e-44b0-830c-544dba8fcf33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_ans2_mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHYrMfKa-CXs"
   },
   "source": [
    "a) The RAG results (research and marketing response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7YM25z4-CXt"
   },
   "outputs": [],
   "source": [
    "test_ans2_eng = final_rag_pipe_eng.invoke(test_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "16nfH9lR-CXt",
    "outputId": "710571a4-3b55-4772-9828-baa911694e1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A common approach is to construct large-scale datasets with diverse tasks and human-written instructions, such as the MEDI dataset, which combines datasets from SuperGLUE and creates additional instructions for tasks. This provides a wide range of contexts and enables models to learn from explicit directives. Another strategy is to use a contrastive loss function during training to encourage embeddings to be similar for inputs with the same instructions and dissimilar for those with different instructions.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans2_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CABfrZ9n-CXt"
   },
   "outputs": [],
   "source": [
    "test_ans2_mkt = final_rag_pipe_mkt.invoke(test_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "iqhjEaCL-CXu",
    "outputId": "30015bd2-e4d3-40fa-d862-175825674dbe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Training data for embedding models that utilize task-specific instructions is often created by combining large datasets from multiple sources and tasks, and then annotating these datasets with human-written instructions. This approach provides a diverse and comprehensive training mixture, enhancing the model's performance on various embedding tasks.\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans2_mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oQaSRdk-CXu"
   },
   "source": [
    "b) The context provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "mfgGy1GA-CXu",
    "outputId": "a7b93547-f8d3-43bc-be2a-de47f4ff448c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'variety of tasks for embedding training with in-\\nstructions. We thus construct a collection of 330\\ndatasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\n\\ncontext learning. INSTRUCTOR significantly out-\\nperforms prior state-of-the-art embedding models\\nby an average of 3.4% over the 70 diverse datasets.\\nINSTRUCTOR also outperforms a variant that is\\ntrained without task instructions (§4), demonstrat-\\ning the importance of instructions to create task-\\n\\nmultitask mixture of 330 diverse datasets with human-written task instructions (MEDI dataset, §2.3). After training\\non MEDI (left), INSTRUCTOR is evaluated on a variety of 70 embedding datasets (66 of which are not seen during\\n\\nfurther training. We first annotate instructions\\nfor 330 diverse tasks and train INSTRUCTOR\\non this multitask mixture with a contrastive loss.\\nWe evaluate INSTRUCTOR on 70 embedding\\nevaluation tasks (66 of which are unseen during\\ntraining), ranging from classification and infor-'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_context_provided(test_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llxtXV8r-CXu"
   },
   "source": [
    "c) The document sources for the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT1inzuV-CXu",
    "outputId": "5f00bc0d-2ec5-47d4-b838-1f661fd8d7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document:  variety of tasks for embedding training with in-\n",
      "structions. We thus construct a collection of 330\n",
      "datasets with instructions across diverse task cate-\n",
      "gories and domains: Multitask Embeddings Data\n",
      "with Instructions (MEDI).\n",
      "Data\n",
      "Construction\n",
      "We\n",
      "build\n",
      "MEDI\n",
      "by\n",
      "combining\n",
      "300\n",
      "datasets\n",
      "from\n",
      "Super-\n",
      "source:  https://arxiv.org/pdf/2212.09741.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n",
      "document:  context learning. INSTRUCTOR significantly out-\n",
      "performs prior state-of-the-art embedding models\n",
      "by an average of 3.4% over the 70 diverse datasets.\n",
      "INSTRUCTOR also outperforms a variant that is\n",
      "trained without task instructions (§4), demonstrat-\n",
      "ing the importance of instructions to create task-\n",
      "source:  https://arxiv.org/pdf/2212.09741.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n",
      "document:  multitask mixture of 330 diverse datasets with human-written task instructions (MEDI dataset, §2.3). After training\n",
      "on MEDI (left), INSTRUCTOR is evaluated on a variety of 70 embedding datasets (66 of which are not seen during\n",
      "source:  https://arxiv.org/pdf/2212.09741.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n",
      "document:  further training. We first annotate instructions\n",
      "for 330 diverse tasks and train INSTRUCTOR\n",
      "on this multitask mixture with a contrastive loss.\n",
      "We evaluate INSTRUCTOR on 70 embedding\n",
      "evaluation tasks (66 of which are unseen during\n",
      "training), ranging from classification and infor-\n",
      "source:  https://arxiv.org/pdf/2212.09741.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_document_sources(test_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLtBoYW8-CXv"
   },
   "source": [
    "d) Also discuss your metric(s) for the first two examples (for both responses) compared to the gold responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYiX5THG-CXv"
   },
   "outputs": [],
   "source": [
    "eng_cos_sim2=get_cosine_similarity(gold_ans2_eng,test_ans2_eng)\n",
    "eng_bert_score2=get_bert_score(gold_ans2_eng,test_ans2_eng)\n",
    "eng_combined_score2=(eng_cos_sim2+eng_bert_score2['f1'])/2\n",
    "\n",
    "mkt_cos_sim2=get_cosine_similarity(gold_ans2_mkt,test_ans2_mkt)\n",
    "mkt_bert_score2=get_bert_score(gold_ans2_mkt,test_ans2_mkt)\n",
    "mkt_combined_score2=(mkt_cos_sim2+mkt_bert_score2['f1'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ig_8waC_-CXv",
    "outputId": "b9301e1b-9253-4fdb-cb4e-6391ee599cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Evaluation:\n",
      "cosine similarity: 0.7304141479983661\n",
      "bert score: {'precision': 0.6751267313957214, 'recall': 0.6467127799987793, 'f1': 0.6606143116950989}\n",
      "combined score: 0.6955142298467325\n",
      "\n",
      "\n",
      "\n",
      "Marketing Evaluation:\n",
      "cosine similarity: 0.9198096650898755\n",
      "bert score: {'precision': 0.7299734950065613, 'recall': 0.6987491250038147, 'f1': 0.7140201330184937}\n",
      "combined score: 0.8169148990541846\n"
     ]
    }
   ],
   "source": [
    "print('Engineering Evaluation:')\n",
    "print('cosine similarity:', eng_cos_sim2)\n",
    "print('bert score:', eng_bert_score2)\n",
    "print('combined score:', eng_combined_score2)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Marketing Evaluation:')\n",
    "print('cosine similarity:', mkt_cos_sim2)\n",
    "print('bert score:', mkt_bert_score2)\n",
    "print('combined score:', mkt_combined_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDovfhEtUMMr"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxDhzMXsT48H"
   },
   "source": [
    "#### 5.2.3 Test Question 3\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WXJX_x3T4we"
   },
   "outputs": [],
   "source": [
    "test_q3=test_questions[83]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSbbSYJB-xsP"
   },
   "source": [
    "a) The RAG results (research and marketing response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR8utn3D-xsP"
   },
   "outputs": [],
   "source": [
    "test_ans3_eng = final_rag_pipe_eng.invoke(test_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "bZe7zu9o-xsQ",
    "outputId": "2f4bf57e-163a-488f-f762-a604b7c09ad2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"A model's ability to answer questions is directly influenced by its exposure to specific question types during training. If a model encounters a question during testing that is identical to one it has seen during training, it can often provide the correct answer through memorization. Models can also select answers from a set of trained responses, even if the question is novel. However, the true test of a model's capabilities is its ability to answer questions with responses that were not explicitly provided in the training data, requiring more complex reasoning and understanding.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans3_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFmSFZcp-xsQ"
   },
   "outputs": [],
   "source": [
    "test_ans3_mkt = final_rag_pipe_mkt.invoke(test_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "jUUtzfpv-xsR",
    "outputId": "45233834-de11-4e73-f454-a306d33daf4c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"A model's ability to answer questions is directly influenced by its exposure to similar questions during training. Models can only provide answers from their training data, so novel questions at test time challenge the model to choose the best response from its limited knowledge base. More complex tasks, like multi-hop QA, require models to reason and interpret, showcasing a higher level of understanding.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans3_mkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVa8fQ77-xsR"
   },
   "source": [
    "b) The context provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "wBlid50E-xsS",
    "outputId": "c3933997-6b34-4d58-a09a-76c499d19143"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A model is able to correctly memorize and respond with the answer to a question that has been seen at training time.\\nA model is able to answer novel questions at test time and choose an answer from the set of answers it has seen during training.\\n\\nA model is able to answer novel questions which have answers not contained in the training dataset.\\n\\n(EM) metric to evaluate model predictions.\\nMulti-Hop Question Answering\\nWe also adopt\\nmore complex QA scenarios, the first of which is\\nmulti-hop QA, where each question q requires rea-\\nsoning over a chain of passages P to obtain the\\ncorrect answer o. For this task, we use the Hot-\\n\\nbetween questions in the train and test sets in several public QA datasets.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_context_provided(test_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rijubtbc-xsS"
   },
   "source": [
    "c) The document sources for the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98nZbcrV-xsS",
    "outputId": "c4ceee5a-d007-4579-915d-3e27aa9e27f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document:  A model is able to correctly memorize and respond with the answer to a question that has been seen at training time.\n",
      "A model is able to answer novel questions at test time and choose an answer from the set of answers it has seen during training.\n",
      "source:  https://lilianweng.github.io/posts/2020-10-29-odqa/\n",
      "doc_source: WWW\n",
      "\n",
      "\n",
      "document:  A model is able to answer novel questions which have answers not contained in the training dataset.\n",
      "source:  https://lilianweng.github.io/posts/2020-10-29-odqa/\n",
      "doc_source: WWW\n",
      "\n",
      "\n",
      "document:  (EM) metric to evaluate model predictions.\n",
      "Multi-Hop Question Answering\n",
      "We also adopt\n",
      "more complex QA scenarios, the first of which is\n",
      "multi-hop QA, where each question q requires rea-\n",
      "soning over a chain of passages P to obtain the\n",
      "correct answer o. For this task, we use the Hot-\n",
      "source:  https://arxiv.org/pdf/2311.08377.pdf\n",
      "doc_source: ArXiv\n",
      "\n",
      "\n",
      "document:  between questions in the train and test sets in several public QA datasets.\n",
      "source:  https://lilianweng.github.io/posts/2020-10-29-odqa/\n",
      "doc_source: WWW\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_document_sources(test_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UZvRmhUOWk"
   },
   "source": [
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44h038A7AUdn"
   },
   "source": [
    "### 5.3 Other Questions\n",
    "\n",
    "Below are a few questions that you should think about. Please answer them in the answer file directly (in a short paragraph) and also see whether they may be relevant for your final write-up.\n",
    "\n",
    "**QUESTION:**\n",
    "\n",
    "5.3.a. How would you expect your response quality to change if you had a chunk size of 50?\n",
    "\n",
    "- If I had a chunk size of 50, I would expect the RAG to provide lower quality responses. With such a small chunk size, there would be more fragmented answers. I also think that the smaller chunks might result in a lot of missed information in the document sources that could be useful for providing relevant responses. This could result in responses that lack the depth and comprehensiveness needed to respond to the questions well.\n",
    "\n",
    "5.3.b. How would you expect your response quality to change if you had a chunk size of 5000?\n",
    "- If I had a chunk size of 5000, I would expect the RAG to provide lower quality answers. With such a large chunk size, there would be too much information overloading the system, and responses might contain irrelevant details unrelated to the question at hand. Overall, there would likely be less clarity and relevance in responses with such large chunk sizes.\n",
    "\n",
    "\n",
    "5.3.c. If you had time, how do you think fine-tuning of the LLM could help?  What type of data would you want for that? And which training approach would you take?\n",
    "- Fine-tuning the LLM could help by improving the relevance of answers and reducing hallucination, given that the data provided is high quality and the training method is appropriate. Speaking of, best data for this fine-tuning task might be pairs of queries and appropriate model responses. For the training approach, I would utilize instruction tuning with the aforementioned labeled datapoints of (prompt, response) pairs, and I would supplement this with Reinforcement learning from human feedback (RLHF), which would seamlessly integrate human feedback from chat conversations.\n",
    "\n",
    "\n",
    "5.3.d. What was your design philosophy  of the prompts? How did they differ between engineering and marketing support?\n",
    "- I designed prompts that were clear indications of the instructions, that repeated the instructions multiple times in different words to push for appropriate responses. The prompts for  engineering support expressed that the audience was already highly technical and knowledgeable in the fundamentals of NLP, so they needed more technical and detailed responses. The prompts for marketing support expressed that the audience was not technical and needed clear and high-level responses that didn't involve too much technical detail. I also asked for a slightly longer answer for engineering than for marketing.\n",
    "\n",
    "5.3.e. What are your average and peak load estimates for the system? Given that, would you suggest a pay-per-use deployment or one that reserves the LLM?\n",
    "- My average load estimate for the system is about 10 queries a day per marketing staff and engineering staff. However, there may be scenarious such as deadlines or campaigns that would spike usage, which leads me to estimate peak loads to be 25 queries, with perhaps 50% of the staff being concurrent users. Overall, given my estimates, I would suggest reseerving the LLM because in such a dynamic working environment the system will likely consistently have high usage and will need to handle peaks regularly.\n",
    "\n",
    "5.3.f. What type of limitations/risks would you see in using this system?\n",
    "- The first risk in using this system is the potential for inaccurate answers or unhelpful answers, which could be attributable to faults in RAG system specifications or faults in the documents that the system has access to. Another limitation has to do with scalability: the computational and financial costs associated with deploying and maintaining such a system at a large scale could be difficult to expend given company resources. Finally, risks related to security may arise because the model may gain access to confidential company information, and a data breach may occur."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
